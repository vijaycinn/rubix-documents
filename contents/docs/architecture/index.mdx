---
title: Architectural Analysis
description: Azure Well-Architected Framework analysis and recommendations for CJN Dakota County RMS
keywords: ["architecture", "azure", "well-architected", "recommendations", "CJN Dakota"]
---

# CJN Dakota County - RMS Architecture

**Date:** February 20, 2026
**Customer:** CJN Dakota County (Tim Anderberg, Nathan Noll)
**Project:** Records Management System (RMS) on Azure Commercial GCC

---

## Executive Summary

<Note type="warning" title="High-Risk Items Requiring Immediate Attention">
- No geo-replication configured (data loss risk)
- Stored credentials in Key Vault instead of Managed Identities (breach risk)
- No documented DR plan (business continuity risk)
- Limited audit logging coverage (CJIS compliance risk)
</Note>

### Well-Architected Framework Maturity

> *Estimated maturity levels based on architectural review. We recommend completing the [Azure Well-Architected Review](https://learn.microsoft.com/assessments/?mode=pre-assessment&session=local) for a formal baseline.*

| Pillar | Current | Target | Gap |
|--------|---------|--------|-----|
| **Reliability** | 6/10 | 8.5/10 | DR strategy, geo-replication, health probes |
| **Security** | 7/10 | 9/10 | Managed identities, Private Link, Always Encrypted |
| **Operational Excellence** | 6.5/10 | 8.5/10 | Distributed tracing, dashboards, runbooks |
| **Performance Efficiency** | 6/10 | 8/10 | Caching, auto-scaling, query optimization |
| **Overall** | **6.4/10** | **8.0/10** | Achievable with phased implementation |

### Architecture Strengths

- PaaS-first approach minimizing operational overhead
- Event-driven architecture using Azure Service Bus with session support
- Multi-tenant isolation through resource groups
- Infrastructure-as-Code with Bicep
- CJIS compliance awareness and security-first design
- Microservices pattern with clear domain separation

---

## 1. System Architecture Overview

<ZoomableImage 
  src="/images/arch/top-down-topology.png" 
  alt="CJN Dakota RMS System Architecture" 
  caption="CJN Dakota RMS System Architecture - Top-down topology showing resource groups, services, and security zones" 
/>

### Architecture Analysis

The system architecture diagram above illustrates the complete Azure-based Records Management System deployed in Azure Commercial GCC to meet CJIS compliance requirements. This topology represents a mature microservices architecture with clear separation of concerns across four distinct resource groups, each serving a specific domain within the RMS ecosystem.

**Resource Group Separation Strategy:** The architecture implements a rigorous isolation model with dedicated resource groups for **RMS Core** (case management and core record operations), **Routing Services** (external system integration and message distribution), **Search Services** (indexing and query operations), and **Shared Infrastructure** (SQL Database, Key Vault, Service Bus Premium). This separation enables independent lifecycle management, granular RBAC assignments, and blast radius containment in the event of a security incident or operational failure.

**CJIS Component Identification:** Services marked with "- CJI" annotations in the topology handle Criminal Justice Information and are subject to CJIS Security Policy 5.0 requirements. These include all App Services (RMS, Routing, Search), Azure Functions, SignalR Hub, Service Bus topics, and the SQL Database. The diagram clearly delineates the trust boundary—all CJI components reside within the VNet-integrated application tier or are accessed via Private Endpoints, ensuring no public internet exposure to sensitive law enforcement data.

**Service Dependencies and Data Flow:** The topology visualizes the hub-and-spoke pattern where the RMS App Service acts as the primary orchestrator, coordinating with Routing and Search services through Azure API Management. All compute services maintain direct connections to Azure SQL Database for transactional consistency, while Key Vault provides centralized secrets management using Managed Identity authentication (eliminating stored credentials). The Service Bus Premium instances (Route_SB and Search_SB) enable asynchronous, event-driven communication between services, supporting session-based ordering per case or destination.

**Edge Security and Gateway Layers:** User requests traverse multiple security checkpoints before reaching application services. Azure Front Door provides global DDoS protection, Web Application Firewall (WAF), and TLS 1.2 termination at the edge. Azure API Management enforces rate limiting, JWT validation, and request throttling before forwarding traffic to backend services. This defense-in-depth approach—detailed further in the **Network Topology & Security Zones** section below—ensures that even if one security layer is compromised, additional controls remain in place.

For detailed capability mapping, refer to the **Key Capabilities** table below. For technology rationale and version specifications, see the **Technology Stack** section. The network security implementation of this architecture is visualized in Section 2.

**Legend:** CJI = Criminal Justice Information | Blue = Data & storage | Green = Compute | Yellow = Integration | Light Blue = Edge

### Technology Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Identity** | Entra ID B2B + Conditional Access | User authentication, MFA |
| **Edge & CDN** | Azure Front Door | Global load balancing, DDoS protection |
| **API Gateway** | Azure API Management | Rate limiting, throttling, API versioning |
| **Web App** | Azure Static Web Apps | Modern SPA hosting |
| **Compute** | Azure App Services + Functions | Microservices (RMS, Routing, Search) |
| **Messaging** | Azure Service Bus (Premium) | Event-driven architecture, session-based messaging |
| **Real-Time** | Azure SignalR Service | Live notifications and updates |
| **Database** | Azure SQL Database (PaaS) | Transactional data with TDE encryption |
| **Caching** | Azure Cache for Redis | Distributed cache for performance |
| **Secrets** | Azure Key Vault Premium | Customer-managed keys, HSM-backed |
| **Observability** | Application Insights + Azure Monitor | Distributed tracing, metrics, logs |
| **Security** | Microsoft Sentinel | SIEM/SOAR for threat detection |
| **Network** | Azure Virtual Network + Private Link | Network isolation, no public internet exposure |
| **IaC** | Bicep + Azure DevOps | Infrastructure as code, CI/CD pipelines |

### Key Capabilities

| Capability | Description |
|-----------|-------------|
| **Secure Data Management** | CJIS-compliant encryption at rest/transit, immutable audit logs, data residency in Azure GCC |
| **Real-Time Operations** | SignalR Hub + Service Bus for live officer notifications and event-driven processing |
| **Enterprise Integration** | 9+ external systems (MNCIS, NIBRS, eCitations, eCharging, MNCrash, MAARC, DPS, Hennepin, LOGIS) |
| **Business Continuity** | SQL geo-replication, automated backup, point-in-time restore, documented DR |
| **Operational Visibility** | Distributed tracing, custom workbooks, Sentinel SIEM, proactive health monitoring |
| **Performance at Scale** | Redis cache, auto-scaling, Front Door CDN, microservices independent scaling |
| **AI-Ready Foundation** | CJIS/non-CJIS data segregation, anonymization pipeline, Azure AI Search readiness |

---

## 1.5 Resource Group Architecture

<ZoomableImage 
  src="/images/arch/RMS%20Resource%20Group-2026-02-23-165317.png" 
  alt="RMS Resource Group Architecture" 
  caption="Resource Group Architecture showing organizational structure and deployment dependencies" 
/>

### Resource Organization and Deployment Strategy

The Resource Group Architecture diagram illustrates the foundational organizational structure that underpins the CJN Dakota RMS deployment, implementing Azure's resource management best practices to enable granular access control, independent lifecycle management, and comprehensive cost allocation. This architecture directly supports the **Multi-tenant isolation through resource groups** strength highlighted in Section 1's Architecture Strengths, establishing clear security boundaries that contain blast radius in the event of service compromise or operational failure.

**Resource Group Separation Rationale:** The architecture implements a four-tier resource group separation strategy, with each group serving a distinct domain: **RMS-Core-RG** (case management microservices, core record operations, and primary transactional workflows), **RMS-Routing-RG** (external system integration services, message routing infrastructure, and partner API gateways), **RMS-Search-RG** (indexing services, query operations, and Azure Cognitive Search resources), and **RMS-Shared-RG** (Azure SQL Database, Azure Key Vault Premium, Service Bus Premium namespaces, and Redis Cache—shared infrastructure accessed by multiple microservices). This separation enables **independent deployment cycles**: the Search team can deploy index schema changes to RMS-Search-RG without risk of impacting core case management functionality in RMS-Core-RG. The isolation also supports **environment-specific scaling**: during peak reporting periods, operators can scale out Routing services in RMS-Routing-RG without incurring cost increases in underutilized Search services. This resource group topology aligns with Azure Landing Zone design patterns, where workload isolation prevents noisy neighbor scenarios and enables targeted disaster recovery (restore only the affected resource group during incident response).

**Tag Strategy for Cost Allocation and Operational Tracking:** The diagram highlights the comprehensive tagging strategy applied to all resources within each resource group: **Environment** (Dev, Stage, Prod), **Owner** (RMS-Team, Routing-Team, Search-Team), **CostCenter** (CC-1001-RMS, CC-1002-Integration, CC-1003-Search), **Project** (CJN-Dakota-RMS), **ComplianceScope** (CJIS-CJI, CJIS-Sensitive, Non-CJIS), **DataClassification** (ConfidentialPII, Restricted, Public), and **DeploymentVersion** (infra-v2.1.0). These tags enable **Azure Cost Management chargeback reports** that break down monthly Azure spend by team, environment, and compliance scope—allowing finance to invoice departments accurately and identify cost optimization opportunities (e.g., "Dev environment in RMS-Routing-RG consuming 30% of total budget—can we downscale non-production tiers?"). The **ComplianceScope** tag feeds into Azure Policy compliance dashboards, validating that all resources tagged as "CJIS-CJI" have mandatory security controls enabled (encryption at rest, Private Link network isolation, diagnostic logging to Log Analytics). The **Owner** tag populates Azure Service Health alert routing, ensuring that when Azure announces planned maintenance for SQL Database in RMS-Shared-RG, the notification is automatically routed to the database administration team via Microsoft Teams webhook. This metadata-driven operational model reduces manual coordination overhead and ensures critical alerts reach the right stakeholders.

**Resource Dependencies and Deployment Order:** The architecture diagram maps explicit dependency relationships between resource groups, establishing a deterministic deployment sequence required for Infrastructure-as-Code (IaC) orchestration. **Phase 1: Shared Infrastructure** (RMS-Shared-RG) must deploy first, provisioning the Azure SQL Database, Key Vault, Service Bus namespaces, and Virtual Network subnets that subsequent phases depend on. **Phase 2: Microservices** (RMS-Core-RG, RMS-Routing-RG, RMS-Search-RG) deploy in parallel once shared infrastructure is healthy, referencing Key Vault secrets via Managed Identity and connecting to SQL Database via Private Endpoints. Each microservice App Service retrieves its connection string from Key Vault during startup, eliminating hardcoded credentials in application configuration. This dependency model is codified in Bicep templates using the `dependsOn` keyword and enforced in Azure DevOps pipelines with sequential deployment stages (detailed in **Appendix B: CI/CD Pipeline Stages**). If a Bicep deployment to RMS-Core-RG fails due to a missing SQL Database connection string, the pipeline halts before attempting RMS-Routing-RG deployment, preventing cascade failures and simplifying rollback procedures.

**RBAC Boundary Enforcement at Resource Group Level:** The diagram illustrates how Azure Role-Based Access Control (RBAC) assignments are scoped to resource groups, implementing the principle of least privilege. **RMS Core Team** members hold **Contributor** role on RMS-Core-RG, granting full management access to App Services, Functions, and Application Insights within that group, but **zero access** to resources in RMS-Routing-RG or RMS-Search-RG. This prevents accidental misconfiguration or malicious lateral movement—a compromised RMS developer account cannot modify Routing infrastructure or access Search service API keys. The **Platform Team** holds **Owner** role on RMS-Shared-RG to manage Key Vault access policies, SQL Database firewall rules, and Service Bus topic configurations, but only **Reader** role on microservice resource groups (visibility for troubleshooting, no modification rights). Break-glass **Global Administrators** receive just-in-time **Owner** access via Azure Privileged Identity Management (PIM) with approval workflow and 4-hour time limits. This RBAC hierarchy directly implements **CJIS Security Policy 5.6 (Personnel Security)** requirements by enforcing separation of duties and limiting blast radius of compromised credentials. The complete RBAC role assignments are detailed in **Appendix A: RBAC Role Matrix**, which maps organizational roles (Patrol Officer, Detective, Supervisor, Admin) to Azure resource permissions. The resource group architecture provides the technical scaffolding for these access controls, ensuring policy enforcement at the Azure Resource Manager layer before requests reach application code.

**Lifecycle Management Advantages:** The resource group structure enables precise control over deployment, scaling, and decommissioning workflows. During major version upgrades, the operations team can **deploy RMS v3.0 to a new parallel resource group** (RMS-Core-V3-RG) while keeping RMS-Core-RG (v2.5) running for A/B testing and gradual traffic migration. Azure Front Door routes 10% of production traffic to the v3 resource group, monitoring error rates and latency metrics before full cutover. If anomalies are detected, operators delete RMS-Core-V3-RG entirely, reverting to the stable v2.5 deployment without complex rollback procedures. This blue-green deployment pattern—enabled by resource group isolation—reduces deployment risk and supports the **Operational Excellence** pillar's recommendation for canary deployments. For cost optimization, non-production environments are scheduled for automatic **resource group suspension**: Azure Automation runbooks execute `Stop-AzWebApp` and `Set-AzSqlDatabase -RequestedServiceObjectiveName 'Basic'` commands on RMS-Core-Dev-RG resources every weeknight at 6 PM, reducing compute costs by 70% during off-hours. The resource group boundary provides a clean scope for bulk operations, avoiding the complexity of targeting individual resources across a sprawling Azure subscription.

---

## 1.6 Multi-Tenant Isolation Strategy

<ZoomableImage 
  src="/images/arch/Multi-Tenant%20Isolation%20Model.png" 
  alt="Multi-Tenant Isolation Model" 
  caption="Multi-tenant isolation architecture with tenant-scoped access control and resource separation" 
/>

### Tenant Provisioning and Isolation Architecture

The Multi-Tenant Isolation Model diagram illustrates the comprehensive tenant onboarding, resource isolation, and data segregation strategy that enables the CJN Dakota RMS to support multiple law enforcement agencies (Dakota County Sheriff, City Police Departments, Regional Task Forces) within a shared Azure infrastructure while maintaining strict security boundaries. This architecture balances operational efficiency (shared infrastructure reduces per-tenant cost) with regulatory compliance (CJIS Security Policy 5.10.1.2 mandates logical separation of CJI data between agencies). The model implements **tenant-scoped access control** at every system layer—Azure Active Directory groups, API Management policies, application-level authorization, and database row-level security—ensuring that an officer from Dakota County Sheriff cannot access cases created by City Police Department, even if application code vulnerabilities exist.

**Tenant Onboarding Process and Provisioning Workflow:** The diagram details the automated tenant provisioning pipeline triggered when a new law enforcement agency contracts for RMS services. The workflow begins with a **Service Request** submitted via Azure DevOps Service Management portal, capturing tenant metadata (Agency Name, ORI Number, Billing Contact, Primary Administrator Email, Azure Region Preference). An Azure DevOps pipeline orchestrates the provisioning sequence: (1) **Entra ID Group Creation** (`RMS-DakotaSheriff-Users`, `RMS-DakotaSheriff-Admins`) with conditional access policies requiring MFA and compliant device enrollment, (2) **Resource Group Deployment** (`RMS-DakotaSheriff-RG`) dedicated to the tenant's application services, following the resource group architecture defined in Section 1.5, (3) **Azure SQL Database Schema Provisioning** (dedicated schema `dakota_sheriff` within the shared multi-tenant SQL Database, with row-level security policies enforcing tenant isolation), (4) **Service Bus Namespace Creation** (dedicated namespace `sb-dakotasheriff-prod` for tenant-specific message routing, preventing cross-tenant message leakage), (5) **Application Configuration** (tenant-specific app settings in Key Vault: `TenantId`, `DatabaseSchema`, `ServiceBusNamespace`, `BillingCode`), and (6) **Admin User Provisioning** (sending onboarding emails with initial credentials, MFA enrollment QR code, and training video links). This automated workflow reduces tenant onboarding time from 5 days (manual provisioning) to 2 hours (pipeline-driven), eliminating human error and ensuring consistent security configurations across all tenants.

**Resource Isolation Guarantees and Dedicated Infrastructure:** The architecture implements a hybrid isolation model combining **shared infrastructure** (cost efficiency) with **dedicated tenant resources** (security boundaries). Each tenant receives a **dedicated resource group** for their application services—tenant-specific App Services, Azure Functions, and Application Insights instances—ensuring compute isolation and preventing one tenant's traffic spike from throttling another tenant's performance. For example, if Dakota County experiences a 10x load increase during a major incident, their dedicated App Service autoscales independently without impacting City Police Department's service availability. However, **shared infrastructure services**—Azure SQL Database (with logical schema separation), Azure Front Door, Azure API Management, and shared Key Vault—are provisioned once and serve all tenants to reduce operational overhead. The diagram illustrates Private Endpoint connections from tenant-specific App Services to the shared SQL Database, with network traffic remaining within the Azure backbone and never traversing public internet. Each tenant's App Service is assigned a **unique Managed Identity** (`RMS-DakotaSheriff-AppService-Identity`), which authenticates to SQL Database and is authorized to access only the `dakota_sheriff` schema via database-level permissions. This identity-based access control ensures that even if a tenant's application code is compromised, the attacker cannot escalate privileges to access other tenants' data at the database layer.

**Performance Isolation Strategy and Resource Quotas:** To prevent noisy neighbor scenarios where one tenant's resource consumption degrades another tenant's performance, the architecture implements **Azure SQL Elastic Pools** with per-database DTU limits and **Service Bus Premium namespaces** with dedicated messaging units per tenant. The diagram shows Dakota County Sheriff's database allocated 50 DTUs within the shared elastic pool (supporting ~500 concurrent users), while smaller agencies receive 10 DTUs (supporting ~100 concurrent users). If Dakota County's workload exceeds 50 DTUs, SQL Database throttles their queries rather than allowing them to starve other tenants' resources. Similarly, each tenant's Service Bus namespace is configured with a quota of 1 Messaging Unit (1 GB memory, 1 vCPU), preventing one tenant from monopolizing the shared Service Bus infrastructure. Azure API Management enforces **per-tenant rate limiting**: Dakota County Sheriff is allocated 1,000 API requests/minute, while smaller agencies receive 200 requests/minute. These quotas are codified in API Management policies using `rate-limit-by-key` with the tenant's `ORI Number` as the throttling key, returning HTTP 429 Too Many Requests when limits are exceeded. Performance metrics are tracked in dedicated Application Insights instances per tenant, enabling tenant-specific performance SLAs (Dakota County contract specifies P95 API latency < 500ms, measured using their dedicated App Insights telemetry). This granular performance isolation—combined with autoscaling configurations that scale tenant resources based on their own workload—ensures predictable performance and meets enterprise SLA commitments.

**Billing and Chargeback Model with Azure Cost Management Tags:** The diagram illustrates the cost allocation strategy that enables accurate per-tenant billing and chargeback to subscribing agencies. All Azure resources are tagged with **TenantId** (e.g., `dakota-sheriff`, `city-police-dept`) and **CostCenter** (e.g., `CC-2001-DakotaSheriff`), feeding into Azure Cost Management reports that break down monthly Azure spend by tenant. Shared infrastructure costs—SQL Database elastic pool base cost, Azure Front Door fees, API Management gateway instance—are **allocated proportionally** based on each tenant's resource consumption metrics (DTU-hours consumed, API requests processed, bandwidth transferred). The billing pipeline executes monthly: (1) Azure Cost Management API queries extract resource-level costs filtered by `TenantId` tag, (2) proportional allocation algorithm distributes shared costs (`SharedCost * (TenantAPIRequests / TotalAPIRequests)`), (3) invoice generation produces PDF invoices with cost breakdown tables (compute, storage, bandwidth, support) per tenant, and (4) automated Azure Automation runbook emails invoices to tenant billing contacts. This transparent cost model ensures accurate chargeback and enables tenants to optimize their usage ("our evidence uploads cost $500/month—can we compress images before upload?"). For tenants with fixed-price contracts, the cost reports serve as internal validation that actual Azure consumption aligns with pricing assumptions.

**Data Isolation at Database Level:** The architecture implements defense-in-depth data isolation using multiple Azure SQL Database security features. **Row-Level Security (RLS) policies** enforce tenant filtering at the database engine level: when Dakota County Sheriff's App Service queries `SELECT * FROM Cases`, SQL Server automatically rewrites the query to `SELECT * FROM Cases WHERE TenantId = 'dakota-sheriff'`, preventing accidental cross-tenant data leakage even if application code omits the `WHERE TenantId` clause. **Separate database schemas** per tenant (`dakota_sheriff`, `city_police_dept`) provide namespace isolation and granular permission management—the `RMS-DakotaSheriff-AppService-Identity` is granted EXECUTE permission on stored procedures in the `dakota_sheriff` schema only, with DENY on all other schemas. **Always Encrypted columns** protect sensitive fields (SSN, criminal history) with tenant-specific Column Encryption Keys (CEKs) stored in Azure Key Vault, ensuring that even a database administrator with full SQL Server access cannot decrypt other tenants' sensitive data. The diagram maps these layered defenses, showing how a compromised application identity must bypass multiple security controls (Managed Identity authentication, schema-level authorization, RLS policy enforcement, Always Encrypted CEKs) before accessing another tenant's data—a defense-in-depth approach aligned with **CJIS Security Policy 5.10 (Information Systems Security Officer)** guidance on multi-tenant system design.

**Scalability Considerations for Multi-Tenant Architecture:** The diagram addresses the scalability challenges inherent in multi-tenant systems, where tenant count growth must not degrade performance or management overhead. The current architecture supports **50 tenants** within a single Azure SQL Database (using elastic pools with 500 DTU capacity) and **20 tenants per Service Bus Premium namespace** (using topic-per-tenant design with 320 topics supported per namespace). When tenant count approaches these limits, the provisioning pipeline automatically **shards tenants across multiple infrastructure instances**: tenants 1-50 route to SQL Database `rms-sql-prod-001`, tenants 51-100 route to `rms-sql-prod-002`. Azure Front Door origin groups distribute traffic to region-specific deployments (East US, West US), enabling geographic scale-out as tenant count grows into hundreds. The architecture also supports **tenant migration**: if a high-volume tenant (Dakota County Sheriff with 5,000 users) outgrows shared infrastructure, operators can promote them to a **dedicated single-tenant deployment** by provisioning a standalone Azure SQL Database and redirecting their traffic via API Management backend pool updates. This hybrid model—starting tenants on shared infrastructure for cost efficiency, graduating high-growth tenants to dedicated infrastructure for performance guarantees—balances operational simplicity with enterprise scalability. The multi-tenant isolation strategy directly cross-references **Section 1.5 Resource Group Architecture**, which provides the resource organization foundation enabling per-tenant resource groups, and supports the **Multi-tenant isolation through resource groups** architecture strength listed in Section 1.

---

## 2. Network Topology & Security Zones

<ZoomableImage 
  src="/images/arch/network-security-zones.png" 
  alt="CJN Dakota Network Security Zones" 
  caption="Network security zones with defense-in-depth architecture and trust boundaries" 
/>

### Network Security Architecture

The network topology diagram illustrates a defense-in-depth security model organized into six distinct trust zones, each with progressively restrictive access controls. This layered approach directly implements **CJIS Security Policy 5.5.4 (Network Security)** requirements by establishing clear trust boundaries, enforcing least-privilege access, and ensuring all CJI data remains within hardened, audited network segments.

**Zone Hierarchy and Trust Model:** Network traffic flows through a graduated trust model, starting from the **Untrusted Zone** (public internet), passing through the **DMZ Edge Layer** (Azure Front Door with WAF/DDoS), entering the **Semi-Trusted API Gateway Zone** (Azure API Management with JWT validation), and finally reaching the **Trusted Application Tier** (VNet-integrated App Services) or **Restricted Data Tier** (Private Endpoint-only data services). The diagram's color coding—red for untrusted, yellow/orange for edge/gateway, green for application tier, and blue for data tier—provides immediate visual clarity on security posture. The **Management Zone** (purple) operates on a separate plane with dedicated RBAC and Privileged Identity Management (PIM) controls.

**Private Endpoint Strategy:** The diagram prominently shows Private Endpoint connections (dark blue arrows) between the Application Tier and Data Tier. All PaaS services handling CJI data—Azure SQL Database, Key Vault Premium, Service Bus Premium, and Blob Storage—are accessed exclusively via Private Endpoints within the `snet-data /24` subnet. This architecture eliminates public IP addresses for data services, ensuring that even compromised application code cannot exfiltrate data directly to the internet. All data plane traffic remains within the Azure backbone network, satisfying CJIS Advanced Authentication requirements.

**Network Security Group (NSG) Flow Control:** Traffic between zones is governed by NSG rules applied at the subnet level. The diagram illustrates filtered communication paths where API Management can route requests to App Services in `snet-rms-app`, `snet-route-app`, and `snet-search-app` subnets, but lateral movement between application subnets is denied by default. Each microservice operates in a network-isolated blast radius, preventing a compromised RMS service from directly accessing Search or Routing infrastructure. NSG flow logs are forwarded to Azure Monitor and Microsoft Sentinel for continuous security monitoring and anomaly detection.

**Zero Trust Network Access (ZTNA):** The architecture implements Zero Trust principles by requiring explicit authentication and authorization at every layer. Even after passing Front Door WAF and API Management JWT validation, application services authenticate to data resources using Managed Identities (visualized as identity-based connections). Telemetry flows (dotted lines) from all services to Azure Monitor enable real-time detection of suspicious network patterns, such as unexpected data access or lateral movement attempts. This telemetry is aggregated in Microsoft Sentinel for Security Information and Event Management (SIEM) correlation, with automated response playbooks for high-severity alerts.

The table below maps each zone to its trust level, security controls, and hosted services. For operational procedures related to network security monitoring, see the **Operational Excellence** section. For application-level security patterns, refer to Section 6: **Defense-in-Depth Security**.

| Zone | Trust Level | Controls | Services |
|------|-------------|----------|----------|
| **Public Internet** | Untrusted | N/A | End users, external partners |
| **DMZ (Edge)** | Low | WAF, DDoS, TLS termination | Front Door, DNS |
| **API Gateway** | Semi-trusted | Rate limiting, JWT, IP filter | API Management |
| **Application Tier** | Trusted | NSGs, VNet integration, managed identity | App Services, Functions, SignalR |
| **Data Tier** | Restricted | Private endpoints only, encryption at rest | SQL, Key Vault, Service Bus, Blob |
| **Management** | Administrative | RBAC, PIM, audit logging | DevOps, Monitor, Sentinel |

---

## 3. Request Lifecycle

<Mermaid
  chart={`sequenceDiagram
    autonumber
    participant User as Officer Browser
    participant EntraID as Entra ID
    participant AFD as Front Door / WAF
    participant SWA as Static Web App
    participant APIM as API Management
    participant RMS as RMS App Service
    participant KV as Key Vault
    participant SQL as Azure SQL
    participant SR as SignalR Hub
    participant AI as App Insights

    User->>EntraID: 1. Login (MFA Required)
    EntraID-->>User: JWT Access Token

    User->>AFD: 2. GET /app (TLS 1.2)
    AFD->>SWA: Route to Static Web App
    SWA-->>User: React SPA Bundle

    User->>AFD: 3. POST /api/cases (Bearer Token)
    AFD->>APIM: Forward Request

    APIM->>APIM: 4. Validate JWT + Rate Limit
    APIM->>RMS: 5. Forward to Backend

    RMS->>AI: Log Request Start
    RMS->>KV: 6. Get Encryption Keys (Managed Identity)
    KV-->>RMS: Column Encryption Keys

    RMS->>SQL: 7. INSERT Case (Always Encrypted)
    SQL-->>RMS: Case ID Returned

    RMS->>SR: 8. Notify Connected Users
    SR-->>User: Real-time Update (WebSocket)

    RMS->>AI: Log Request Complete
    RMS-->>APIM: 9. HTTP 201 Created
    APIM-->>AFD: Response
    AFD-->>User: Case Created`}
/>

| Step | Operation | Target Latency | SLA |
|------|-----------|---------------|-----|
| 1 | Authentication (Entra ID + MFA) | < 500ms | 99.99% |
| 2-3 | Front Door routing | < 50ms | 99.99% |
| 4 | API Management processing | < 100ms | 99.95% |
| 5-7 | Business logic + DB write | < 500ms | 99.9% |
| 8 | SignalR broadcast | < 200ms | 99.9% |
| **Total** | **End-to-end** | **< 1.5s** | **99.9%** |

### Distributed Tracing and Monitoring

<ZoomableImage 
  src="/images/arch/Azure%20Application%20Insights-2026-02-23-170842.png" 
  alt="Azure Application Insights Distributed Tracing" 
  caption="Application Insights distributed tracing showing correlation IDs and dependency tracking" 
/>

#### Operational Visibility and Performance Monitoring

The Application Insights distributed tracing diagram visualizes the observability infrastructure that provides end-to-end visibility into request flows, dependency tracking, and performance anomaly detection across the entire CJN Dakota RMS distributed system. This monitoring architecture is foundational to achieving the **Operational Excellence** targets defined in Section 1's maturity assessment, enabling proactive incident response and data-driven capacity planning. The implementation aligns with the Phase 3 Operations roadmap item to "implement comprehensive distributed tracing with Application Insights" and establishes the baseline metrics referenced in **Appendix C (Monitoring Alert Thresholds)**.

**Distributed Tracing Implementation Across Services:** The diagram illustrates how Application Insights SDKs embedded in each service—RMS App Service, Routing App Service, Search App Service, Azure Functions, and APIM Gateway—automatically capture telemetry data and propagate correlation context using the W3C Trace Context standard (`traceparent` HTTP header). When an officer initiates a "Create Case" request, the Static Web App generates a root `TraceId` and includes it in the API call to Azure Front Door. Front Door forwards the trace header to APIM, which appends its own `SpanId` and forwards to the RMS App Service. The RMS service creates child spans for each operation: Key Vault secret retrieval, SQL Database INSERT, Service Bus message publish, and SignalR notification broadcast. These nested spans form a hierarchical **dependency graph** in Application Insights, enabling operators to drill down from a slow API response (e.g., 5-second latency) to identify the specific bottleneck (e.g., SQL query took 4.8 seconds due to missing index). This granular visibility—unavailable in traditional monolithic application monitoring—is essential for troubleshooting distributed system performance issues.

**Correlation ID Propagation Through the Request Chain:** The diagram emphasizes the role of `CorrelationId` (synonymous with `TraceId`) as the thread connecting telemetry across disparate services and asynchronous workflows. When the RMS service publishes a `CaseCreated` event to Service Bus, it includes the `CorrelationId` in message custom properties. Downstream consumers—`func-search-indexer`, `func-mncis-router`—extract this property and set it as the Application Insights `Operation_ParentId`, linking their telemetry to the original user request. This correlation extends even to external API calls: when `func-mncis-router` invokes the MNCIS SOAP endpoint, Application Insights tracks the outbound HTTP dependency with the same `CorrelationId`, creating a complete trace from browser click to external system response. Operators can query Application Insights Analytics with `Operation_Id == '...'` to retrieve all telemetry events (requests, dependencies, exceptions, custom events) for a single user transaction, dramatically reducing mean time to resolution (MTTR) for production incidents. For CJIS compliance, correlation IDs are also written to the SQL audit log, enabling cross-referencing between Application Insights performance data and regulatory audit trails.

**Dependency Tracking Configuration for SQL, Service Bus, and Redis:** The diagram maps Application Insights' automatic dependency tracking capabilities for Azure PaaS services. The `Microsoft.ApplicationInsights.DependencyCollector` NuGet package intercepts ADO.NET database calls, Azure Service Bus SDK operations, and StackExchange.Redis cache commands, capturing dependency telemetry without code modification. For SQL Database, Application Insights logs each query with duration, row count, success/failure status, and the full SQL command text (sanitized to remove parameter values per CJIS requirements). For Service Bus, it tracks message publish operations with topic name, message size, and latency. For Redis, it captures cache hit/miss rates and command execution times. This dependency telemetry feeds into Application Insights' **Application Map** visualization (shown in the diagram), which renders real-time topology graphs with color-coded health indicators: green nodes (< 100ms average), yellow nodes (100-500ms), red nodes (> 500ms or high error rate). The Application Map serves as the primary operational dashboard—operators glance at the map to identify degraded services and receive drill-down links to diagnostic logs.

**Performance Baselines and Anomaly Detection:** The architecture implements Application Insights Smart Detection, which uses machine learning to establish performance baselines for each service and alert on anomalies. For example, if the RMS API's median response time is typically 350ms but suddenly spikes to 2 seconds, Smart Detection automatically creates an incident with root cause analysis: "SQL Database response time increased 6x—possible query regression or missing index." The diagram shows integration with Azure Monitor Action Groups, which route anomaly alerts to Microsoft Teams channels for immediate triage and optionally trigger auto-remediation runbooks (e.g., scaling out the App Service plan). Smart Detection also identifies dependency performance degradation ("MNCIS API calls from func-mncis-router are 3x slower than last week") and memory leak patterns ("RMS App Service memory usage increasing 10% per hour"). These proactive alerts—combined with the alert thresholds defined in Appendix C—enable the operations team to address performance regressions before they impact end users.

**Alert Rules for Critical Metrics and Forward References:** The diagram highlights configured Azure Monitor alert rules that trigger on key performance indicators: **API response time** (warning at 2s, critical at 5s), **error rate** (warning at 1%, critical at 5%), **Service Bus DLQ depth** (critical at 10 messages), and **SQL DTU utilization** (warning at 70%, critical at 90%). Each alert rule specifies an Action Group with notification channels (email, SMS, Teams webhook) and severity levels (Sev 0-4). The monitoring strategy documented here references **Appendix C (Monitoring Alert Thresholds)** for complete threshold definitions and remediation procedures. This comprehensive alerting framework directly supports the **Operational Excellence** pillar recommendations from Section 1, addressing the maturity gap from 6.5/10 to 8.5/10. The distributed tracing capability is also identified as a Phase 3 deliverable in the Operations roadmap, providing the telemetry foundation for future AI-driven incident prediction and automated root cause analysis.

---

## 4. Event-Driven Processing

### 4.1 Service Bus Topology

<ZoomableImage 
  src="/images/arch/Azure%20Service%20Bus%20Event%20Flow-2026-02-23-165619.png" 
  alt="Azure Service Bus Event Flow Architecture" 
  caption="Service Bus topology with session-based ordering and pub/sub pattern" 
/>

#### Event-Driven Architecture Analysis

The Service Bus topology diagram illustrates a mature publish/subscribe (pub/sub) event-driven architecture that decouples microservices and enables asynchronous, scalable processing of RMS operations. This design pattern is fundamental to achieving high availability, fault tolerance, and independent service scaling across the RMS, Routing, and Search domains.

**Session-Based Ordering Guarantees:** The diagram highlights three primary topics—`case-events`, `route-commands`, and `search-index`—each configured with **Service Bus Sessions** to guarantee FIFO (first-in, first-out) ordering within a session context. For `case-events`, the session key is `CaseId`, ensuring all events for Case #12345 are processed sequentially even under high load. Similarly, `route-commands` uses `DestinationId` as the session key, guaranteeing that all messages destined for MNCIS are processed in order, preventing race conditions where a case update could arrive at the state system before the initial case creation. This session-based ordering is critical for maintaining data consistency with external partners who expect deterministic event sequences.

**Topic/Subscription Filtering Pattern:** The architecture leverages Azure Service Bus's content-based filtering to route messages from a single `route-commands` topic to multiple specialized subscribers. As shown in the diagram, subscriptions like `mncis-router`, `nibrs-router`, `ecite-router`, `maarc-router`, and `crash-router` each apply a SQL filter expression (`dest = 'MNCIS'`, `dest = 'NIBRS'`, etc.) to receive only relevant messages. This fan-out pattern eliminates the need for hardcoded routing logic in publisher code—the RMS API Service simply publishes a message with a `Destination` property, and Service Bus automatically delivers it to the appropriate consumer. This design supports rapid addition of new integration partners without modifying existing services.

**Dead Letter Queue (DLQ) Monitoring and Recovery:** The diagram prominently features the error handling flow where failed messages are routed to a Dead Letter Queue after 10 delivery attempts (configured in the table below). The `DLQ Processor Function` continuously monitors the DLQ depth and triggers an **Azure Monitor Alert** when the count exceeds 10 messages, indicating a systemic integration failure. Operational procedures for DLQ triage involve inspecting message metadata (exception details, retry count) in the Azure Portal, correcting the root cause (e.g., restoring a downed external API), and manually resubmitting messages via the `ServiceBusAdministrationClient` SDK. This manual gate prevents automatic retry storms that could overwhelm partner systems.

**Microservices Decoupling and Independent Scaling:** By using Service Bus as an intermediary, producer services (RMS API, Routing API, Scheduled Jobs) remain completely unaware of consumer implementations. If the `func-nibrs-sender` Function experiences a temporary outage, messages accumulate in the `nibrs-router` subscription without blocking other integrations. Each consumer Function scales independently based on queue depth, enabling cost-optimized scaling where high-volume integrations (MNCIS) run on multiple instances while low-volume integrations (MNCrash) use a single instance. This architecture prevents monolithic bottlenecks and supports continuous deployment of individual services without system-wide downtime.

The **Service Bus Premium** tier (selected for this deployment) provides essential enterprise features: geo-disaster recovery, Private Link network isolation (see Section 2), dedicated compute capacity (1 Messaging Unit = 1 vCPU), and larger message sizes (100 MB vs. 256 KB in Standard). For configuration details, see the property table below. For resilience patterns applied to external API calls within consumer Functions, see Section 5: **External Integrations**.

| Property | Value | Rationale |
|----------|-------|-----------|
| **Tier** | Premium | Geo-DR, Private Link, dedicated resources |
| **Sessions** | Enabled (per topic) | Ordered processing per case/destination |
| **Max Delivery** | 10 attempts | Exponential backoff before DLQ |
| **Lock Duration** | 5 minutes | Sufficient for external API calls |
| **TTL** | 24 hours | Prevent stale message processing |
| **DLQ Monitoring** | Alert on count > 10 | Immediate ops notification |

### 4.2 RMS Event Publishing

<ZoomableImage 
  src="/images/arch/Azure%20RMS%20Services%20Event-2026-02-23-165517.png" 
  alt="Azure RMS Services Event Publishing" 
  caption="Event publishing architecture showing event types, schemas, and correlation strategy" 
/>

#### Event Publishing Architecture

The RMS Event Publishing diagram illustrates the comprehensive event taxonomy and publishing mechanisms used across the RMS, Routing, and Search microservices to maintain eventual consistency and enable reactive workflows throughout the distributed system. This architecture implements the event sourcing pattern for critical state changes, creating an immutable audit trail that can reconstruct system state at any point in time and support future AI/ML pipelines for predictive analytics.

**Event Type Classification and Schema Design:** The diagram categorizes published events into four primary types: **case-events** (CaseCreated, CaseUpdated, CaseStatusChanged, EvidenceAttached), **route-commands** (SubmitToMNCIS, SendNIBRSReport, FileCitation), **search-index** (IndexCase, UpdateSearchDocument, DeleteFromIndex), and **audit-events** (UserAction, SystemAction, ComplianceLog). Each event type follows a standardized JSON schema with required properties: `EventId` (GUID for deduplication), `EventType` (fully qualified type name), `AggregateId` (CaseId, EvidenceId, etc.), `Timestamp` (ISO 8601 UTC), `CorrelationId` (distributed tracing identifier), `UserId` (actor for CJIS audit), `Payload` (strongly-typed domain data), and `SchemaVersion` (semantic versioning for backward compatibility). This schema standardization enables generic event processing infrastructure—consumers can deserialize the envelope without knowing payload specifics, logging correlation IDs before payload parsing for observability.

**Correlation ID Strategy for Distributed Tracing:** Every event published to Service Bus includes a `CorrelationId` property that propagates through the entire processing chain, enabling end-to-end request tracing across microservice boundaries. When an officer submits a case, the RMS API generates a root `TraceId` (W3C Trace Context standard) and embeds it in the `case-events` message. Downstream consumers (Search indexer, MNCIS router) extract this `CorrelationId`, attach it to their Application Insights telemetry, and include it in subsequent Service Bus messages. This creates a complete dependency graph in Application Insights, allowing operators to visualize the full event cascade triggered by a single user action. For example, querying Application Insights for `CorrelationId = abc123` reveals: API request → case-events published → search indexer invoked → route-commands published → MNCIS Function called → MNCIS API response—with timestamps and latencies at each hop. This tracing capability is essential for troubleshooting production incidents and identifying bottlenecks in the event processing pipeline.

**Event Versioning for Backward Compatibility:** The diagram highlights the `SchemaVersion` property (e.g., `CaseCreated_v2`) used to implement zero-downtime schema evolution. When a new property is added to the `CaseCreated` event payload (e.g., adding `OfficerBadgeNumber` for enhanced audit logging), publishers increment the version to `v2` while consumers maintain support for both `v1` and `v2` schemas using polymorphic deserialization. Older consumers that don't recognize `v2` fall back to processing the common `v1` properties, preventing breaking changes during rolling deployments. This versioning strategy—combined with Azure Service Bus's native support for message properties—enables the RMS team to evolve the event schema incrementally without coordinating simultaneous deployments of all producer and consumer services. The Architecture Decision Record (ADR) for this pattern references Martin Fowler's event versioning guidance and prioritizes additive changes (new optional properties) over breaking changes (renaming/removing properties).

**Publisher Pattern Implementation Across Services:** The diagram maps event publishing responsibilities to specific services: **RMS API Service** publishes `case-events` and `audit-events` after successful database writes (using the Outbox pattern to guarantee at-least-once delivery even if Service Bus is unavailable), **Routing API Service** publishes `route-commands` based on configuration rules ("new felony cases automatically route to MNCIS"), and **Search Service** publishes `search-index` events when document updates are required. Each service uses the Azure Service Bus SDK's `ServiceBusClient` with connection strings retrieved from Key Vault via Managed Identity, publishing to dedicated topics with explicit partition keys (for `case-events`, the partition key is `CaseId % 10` to distribute load across 10 partitions). This topic-per-event-type design (rather than a single monolithic event bus) enables granular access control—the Search Service can publish to `search-index` but cannot publish to `route-commands`, reducing blast radius in the event of a compromised service.

### 4.3 Event Consumption and Processing

<ZoomableImage 
  src="/images/arch/Event%20Processing%20and-2026-02-23-170025.png" 
  alt="Event Processing and Consumption Architecture" 
  caption="Event consumption architecture with Azure Functions showing error handling and idempotency patterns" 
/>

#### Consumer Architecture and Processing Guarantees

The Event Consumption and Processing diagram illustrates how Azure Functions consume messages from Service Bus subscriptions, implementing robust error handling, idempotency patterns, and performance optimization strategies to guarantee exactly-once processing semantics for critical workflows. This architecture balances throughput requirements (processing 50,000+ events/day during peak hours) with reliability requirements (zero data loss, deterministic retry behavior) while maintaining CJIS audit compliance for all CJI event processing.

**Azure Function Triggers and Bindings Configuration:** The diagram shows dedicated Azure Functions for each event subscription: `func-case-processor` (consumes `case-events` subscription), `func-mncis-router` (consumes `mncis-router` subscription), `func-search-indexer` (consumes `search-index` subscription), and `func-audit-logger` (consumes `audit-events` subscription). Each Function uses the `ServiceBusTrigger` attribute with session-enabled configuration to process messages sequentially per session: `[ServiceBusTrigger("case-events", "case-processor", IsSessionsEnabled = true)]`. The `host.json` configuration specifies critical performance parameters: `maxConcurrentSessions = 8` (parallel session processors per instance), `maxConcurrentCalls = 1` (sequential processing within a session for ordering guarantee), `prefetchCount = 32` (message pre-fetching for reduced latency), and `autoCompleteMessages = false` (explicit completion control for error handling). This configuration strikes a balance—8 concurrent sessions provide horizontal scaling while `maxConcurrentCalls = 1` ensures FIFO ordering within each case.

**Error Handling and Retry Patterns with Exponential Backoff:** The diagram details the multi-layered error handling strategy. When a Function encounters a transient exception (network timeout, SQL deadlock, external API 503), the Service Bus redelivery mechanism automatically retries the message with exponential backoff: 1st retry after 10 seconds, 2nd after 40 seconds, 3rd after 90 seconds (backoff multiplier = 2.0, capped at 5 minutes). The Function code wraps external calls in Polly policies (see Section 5 code examples for circuit breaker implementation) to handle partner API failures gracefully. For poison messages that fail after 10 retry attempts—such as a malformed JSON payload or a permanent business rule violation—the message is automatically routed to the Dead Letter Queue (DLQ), where the `func-dlq-processor` Function logs the failure details to Application Insights with full context (stack trace, message properties, retry history) and sends an alert to the operations team via Azure Monitor Action Group. This graduated retry strategy prevents transient failures from triggering DLQ alarms while ensuring persistent errors receive immediate attention.

**Performance and Scaling Considerations:** The architecture leverages **Consumption Plan** for low-traffic Functions (DLQ processor, audit logger) to minimize costs, but deploys high-throughput consumers (case processor, MNCIS router) on **Premium Plan** for guaranteed compute capacity and faster cold start times (< 1 second vs. 10+ seconds on Consumption Plan). Premium Plan Functions also benefit from VNet integration for Private Link connectivity to Service Bus (avoiding public internet traversal) and from always-ready instances (minimum 1 instance pre-warmed, scaling to 20 instances under load). The diagram shows Azure Functions scaling logic: when Service Bus queue depth exceeds 1,000 messages per instance, the platform automatically provisions additional Function instances (up to the configured maximum of 20). This elastic scaling—combined with Service Bus Premium's dedicated messaging units—ensures the system maintains sub-second processing latency even during incident spikes (e.g., 500 officers simultaneously creating cases after a major event).

**Idempotency Patterns for Exactly-Once Processing Guarantee:** To prevent duplicate processing when Service Bus retries a message, each Function implements idempotency checks using distributed caching. Before processing a `CaseCreated` event, `func-case-processor` queries Azure Cache for Redis with the key `processed:{EventId}` (TTL = 7 days, matching Service Bus retention). If the key exists, the Function immediately completes the message without re-execution, logging a "duplicate message detected" telemetry event. For new messages, the Function performs a **two-phase commit**: (1) write to Redis cache with the processing status "in-progress", (2) execute business logic and persist to SQL Database, (3) update Redis to "completed" and complete the Service Bus message. If the Function crashes between steps 1 and 3, Service Bus redelivers the message, but the Redis check detects "in-progress" status and safely completes the message (the SQL write may have succeeded before the crash). This idempotency pattern—combined with SQL Database's deterministic stored procedures (checking for existing CaseId before INSERT)—guarantees exactly-once processing semantics without requiring distributed transactions or two-phase commit protocols across Service Bus and SQL Database.

**Message Session Handling for Ordered Processing:** The diagram emphasizes Service Bus Sessions as the mechanism for maintaining event ordering within a case context. Each `case-events` message includes a `SessionId` property set to the `CaseId` value. Service Bus guarantees that all messages with the same `SessionId` are delivered to a single Function instance in FIFO order, preventing race conditions where a `CaseUpdated` event could be processed before the corresponding `CaseCreated` event. The Function code accepts the session using `IMessageSession.AcceptSessionAsync(sessionId)`, processes messages sequentially, and releases the session lock upon completion. This session-based ordering is critical for maintaining referential integrity in the search index—if `CaseUpdated` processed before `CaseCreated`, the search indexer would attempt to update a non-existent document. For non-case events (e.g., system health checks), messages are published without a `SessionId`, allowing parallel processing across all available Function instances for maximum throughput.

---

## 5. External Integrations

<ZoomableImage 
  src="/images/arch/API%20Gateway%20to%20Outbound-2026-02-23-165702.png" 
  alt="API Gateway to Outbound External Systems" 
  caption="External integration architecture showing inbound/outbound flows and resilience patterns" 
/>

### External Integration Architecture

The external integrations diagram maps the complete data flow for bidirectional communication between the CJN Dakota RMS and nine external law enforcement, judicial, and public safety systems. This architecture handles both **inbound data** (court returns from MNCIS, mobile citations from Citation App) and **outbound data** (case submissions, crash reports, FBI reporting), implementing rigorous resilience patterns to ensure guaranteed delivery even when partner systems experience downtime.

**Integration Gateway and Security Boundary:** All external traffic enters through the Azure API Management (APIM) Gateway, which serves as the single ingress/egress point for partner communications. The diagram highlights APIM's critical security functions: TLS 1.2/1.3 encryption for data in transit, JWT token validation for authenticated partners, IP allowlist filtering (restricting MNCIS to known state IP ranges), and comprehensive request/response logging for audit trails. APIM policies also enforce rate limiting (e.g., 100 requests/minute per partner) and throttling to protect backend services from denial-of-service scenarios. This gateway isolation ensures that external partners never obtain direct network access to RMS application or data tiers.

**Inbound Processing Pipeline:** The diagram shows two primary inbound flows: **MNCIS Inbound** (court returns, charging decisions, case status updates) and **Citation App** (mobile-generated traffic citations). Inbound requests are routed from APIM to the **ROUTE_IN Service**, which performs schema validation, deduplication (checking for duplicate submission IDs), and message transformation into the internal RMS domain model. The processing pipeline includes scheduled jobs (MN Crash Job, MAARC Job, CAD Job) that poll external endpoints for updates when push-based webhooks are unavailable. These jobs publish processed data to the **RMS Service Core**, which persists entities to the **RMS Database** and triggers downstream workflows via Service Bus (see Section 4).

**Outbound Routing via ROUTE2DEST Service:** The **ROUTE2DEST Distribution Service** (highlighted in purple) acts as the central hub for all outbound integrations, consuming messages from the `route-commands` Service Bus topic and invoking partner-specific Azure Functions. The diagram illustrates six outbound destinations: **MNCIS_OUT** (state judicial system), **eCitations_OUT** (citation filings), **eCharging_OUT** (prosecutor charging documents), **MNCrash_OUT** (traffic crash reports), **MAARC_OUT** (multi-agency records consortium), and **NIBRS_OUT** (National Incident-Based Reporting System for FBI). Each integration implements the resilience patterns detailed in the **Integration Resilience Patterns** subsection below—exponential backoff retry (3 attempts with 2^n second delays), circuit breaker (opens after 3 consecutive failures for 1 minute cooldown), and Dead Letter Queue routing after 10 total failures.

**Cross-System Data Lineage and Audit Compliance:** The architecture maintains end-to-end traceability for all external data exchanges. Every message flowing through ROUTE2DEST includes distributed tracing headers (W3C Trace Context standard) that correlate with Application Insights telemetry, enabling operators to trace a case submission from the officer's browser through the RMS API, Service Bus, ROUTE2DEST, and ultimately to the MNCIS API response. This telemetry is critical for troubleshooting integration failures (e.g., "Why didn't case #12345 reach MNCIS?") and for regulatory compliance (CJIS Security Policy 5.4.1 requires audit logs for all CJI disclosures). Failed messages in the Dead Letter Queue are logged with full context (exception stack traces, retry count, original message payload) for forensic analysis.

For detailed partner-specific protocols and data formats, refer to the **External Systems** table below. For code examples demonstrating Polly-based retry and circuit breaker policies, see the **Integration Resilience Patterns** tab section. This integration architecture directly supports the multi-agency collaboration capabilities listed in Section 1's **Key Capabilities** table.

### External Systems

| System | Direction | Data Type | Protocol |
|--------|-----------|-----------|----------|
| **MNCIS** | Bidirectional | Case submissions, court returns | HTTPS/API |
| **NIBRS** | Outbound | FBI crime reporting (monthly/annual) | NIBRS XML |
| **eCitations** | Outbound | Citation filings | HTTPS/API |
| **eCharging** | Outbound | Charging documents to prosecutors | HTTPS/API |
| **MNCrash** | Outbound | Crash reports | HTTPS/API |
| **MAARC** | Bidirectional | Multi-agency shared records | HTTPS/API |
| **DPS** | Outbound | Dept. of Public Safety data | HTTPS/API |
| **Hennepin County** | Bidirectional | County data exchange | HTTPS/API |
| **LOGIS** | Bidirectional | Shared government IT services | HTTPS/API |
| **Citation App** | Inbound | Mobile-generated citations | HTTPS/API |

### Integration Resilience Patterns

| Pattern | Implementation | Behavior |
|---------|---------------|----------|
| **Retry (Exponential Backoff)** | Polly `WaitAndRetryAsync` | 3 retries at 2^n seconds |
| **Circuit Breaker** | Polly `CircuitBreakerAsync` | Opens after 3 failures, 1 min break |
| **Guaranteed Delivery** | Service Bus Topics | 10 retry attempts before DLQ |
| **Dead Letter Queue** | DLQ Processor Function | Alert on count > 10, manual reprocess |
| **Store and Forward** | Fallback mechanism | Persist locally if partner unavailable |

<Tabs defaultValue="overview">
  <TabsList>
    <TabsTrigger value="overview">Overview</TabsTrigger>
    <TabsTrigger value="code">Code Examples</TabsTrigger>
  </TabsList>
  <TabsContent value="overview">
    Each external integration has a dedicated Azure Function with per-partner retry policies, circuit breakers, and dead-letter queue handling. All integrations use TLS 1.2+ and are instrumented with Application Insights distributed tracing.
  </TabsContent>
  <TabsContent value="code">
    ```csharp
    // Retry Policy with Exponential Backoff
    var retryPolicy = Policy
        .Handle<HttpRequestException>()
        .Or<TimeoutException>()
        .WaitAndRetryAsync(
            retryCount: 3,
            sleepDurationProvider: retryAttempt =>
                TimeSpan.FromSeconds(Math.Pow(2, retryAttempt)),
            onRetry: (exception, timeSpan, retryCount, context) => {
                _logger.LogWarning($"Retry {retryCount} after {timeSpan.TotalSeconds}s");
            });

    // Circuit Breaker Pattern
    var circuitBreakerPolicy = Policy
        .HandleResult<HttpResponseMessage>(r => !r.IsSuccessStatusCode)
        .Or<HttpRequestException>()
        .CircuitBreakerAsync(
            handledEventsAllowedBeforeBreaking: 3,
            durationOfBreak: TimeSpan.FromMinutes(1),
            onBreak: (result, timespan) => {
                _logger.LogError($"Circuit breaker opened for {timespan.TotalMinutes} min");
            },
            onReset: () => {
                _logger.LogInformation("Circuit breaker reset - system recovered");
            });
    ```
  </TabsContent>
</Tabs>

---

## 6. Defense-in-Depth Security

<Mermaid
  chart={`graph TB
    subgraph L1["LAYER 1: IDENTITY AND ACCESS"]
      direction LR
      EntraID["Entra ID B2B"]
      MFA["Multi-Factor Auth<br/>FIPS 140 Validated"]
      CA["Conditional Access"]
      PIM["Privileged Identity Mgmt"]
      RBAC["Role-Based Access"]
    end

    subgraph L2["LAYER 2: PERIMETER"]
      direction LR
      AFD_WAF["Azure Front Door WAF"]
      DDoS["DDoS Protection Standard"]
      NSG_Edge["NSG: IP Allowlisting"]
    end

    subgraph L3["LAYER 3: NETWORK"]
      direction LR
      VNet["Virtual Network 10.0.0.0/16"]
      NSG_App["NSG: App Subnets"]
      PrivLink["Private Link"]
      PrivDNS["Private DNS Zones"]
    end

    subgraph L4["LAYER 4: APPLICATION"]
      direction LR
      APIM_SEC["APIM: JWT + Rate Limit"]
      InputVal["Input Validation"]
      TLS["TLS 1.2 Only"]
    end

    subgraph L5["LAYER 5: COMPUTE"]
      direction LR
      ManagedID["Managed Identities"]
      AppSec["Key Vault References"]
      Updates["PaaS Auto-Patching"]
    end

    subgraph L6["LAYER 6: DATA"]
      direction LR
      TDE["SQL TDE + CMK"]
      AlwaysEnc["Always Encrypted PII"]
      BlobEnc["Blob Double Encryption"]
      Backup["Immutable Backups"]
    end

    subgraph L7["LAYER 7: MONITORING"]
      direction LR
      Sentinel_SIEM["Microsoft Sentinel SIEM"]
      Defender["Defender for Cloud"]
      AuditLog["7-Year Audit Retention"]
      Lockbox["Customer Lockbox"]
    end

    L1 --> L2
    L2 --> L3
    L3 --> L4
    L4 --> L5
    L5 --> L6
    L6 --> L7

    style L1 fill:#dbeafe,stroke:#1d4ed8,stroke-width:3px
    style L2 fill:#fef3c7,stroke:#b45309,stroke-width:3px
    style L3 fill:#dcfce7,stroke:#16a34a,stroke-width:3px
    style L4 fill:#fce7f3,stroke:#be185d,stroke-width:3px
    style L5 fill:#e0e7ff,stroke:#4338ca,stroke-width:3px
    style L6 fill:#f3e8ff,stroke:#7c3aed,stroke-width:3px
    style L7 fill:#fef2f2,stroke:#dc2626,stroke-width:3px`}
/>

### Security Controls Matrix

| Layer | Control | CJIS Mapping | Status |
|-------|---------|-------------|--------|
| **Identity** | Entra ID + MFA | 5.6.2.2 | Implemented |
| **Identity** | FIPS 140 Authenticators | 5.6.2.2 | Needs Validation |
| **Identity** | PIM/JIT Access | 5.5.2 | Recommended |
| **Perimeter** | WAF v2 Rules | 5.10.4 | Implemented |
| **Perimeter** | DDoS Protection | 5.10.4 | Implemented |
| **Network** | Private Link | 5.5.4 | Recommended |
| **Network** | VNet + NSGs | 5.5.4 | Implemented |
| **Application** | JWT Validation | 5.5.2 | Implemented |
| **Application** | Rate Limiting | 5.10.4 | Implemented |
| **Compute** | Managed Identities | 5.6.1 | Recommended |
| **Data** | TDE + CMK | 5.10.1 | Recommended |
| **Data** | Always Encrypted | 5.10.1 | Recommended |
| **Monitoring** | Sentinel SIEM | 5.4.1 | Recommended |
| **Monitoring** | Audit Logging (7 yr) | 5.4 | Partial |

### Security Recommendations

<Tabs defaultValue="recommendations">
  <TabsList>
    <TabsTrigger value="recommendations">Recommendations</TabsTrigger>
    <TabsTrigger value="bicep">Bicep Examples</TabsTrigger>
  </TabsList>
  <TabsContent value="recommendations">
    | Recommendation | Current State | Target State | Effort | CJIS Ref |
    |---------------|-------------|-------------|--------|----------|
    | **Managed Identities** | Credentials in Key Vault | System Assigned MI everywhere | 2-3 weeks | 5.10 |
    | **TDE with CMK** | Default encryption | Customer-managed keys via KV Premium | 2 weeks | 5.10.1 |
    | **Private Link** | Public endpoints + firewall | No public endpoints | 3-4 weeks | 5.5.4 |
    | **Always Encrypted** | At-rest encryption only | Column-level PII encryption | 3 weeks | 5.10 |
    | **SQL Audit Logs** | Basic monitoring | Immutable storage, 7-year retention | 1 week | 5.4 |
    | **Sentinel SIEM** | Basic Azure Monitor | Full SIEM/SOAR with playbooks | 3-4 weeks | 5.4.1 |
  </TabsContent>
  <TabsContent value="bicep">
    ```bicep
    // Managed Identity for Azure Function
    resource functionApp 'Microsoft.Web/sites@2023-01-01' = {
      name: 'func-rms-${environment}'
      identity: {
        type: 'SystemAssigned'
      }
    }

    resource sqlAdmin 'Microsoft.Sql/servers/administrators@2023-05-01-preview' = {
      parent: sqlServer
      name: 'ActiveDirectory'
      properties: {
        administratorType: 'ActiveDirectory'
        login: functionApp.name
        sid: functionApp.identity.principalId
        tenantId: tenant().tenantId
      }
    }
    ```

    ```bicep
    // Private Endpoint for SQL Database
    resource privateEndpoint 'Microsoft.Network/privateEndpoints@2023-05-01' = {
      name: 'pe-sql-${environment}'
      location: location
      properties: {
        subnet: { id: sqlSubnetId }
        privateLinkServiceConnections: [{
          name: 'sql-connection'
          properties: {
            privateLinkServiceId: sqlServer.id
            groupIds: ['sqlServer']
          }
        }]
      }
    }
    ```
  </TabsContent>
</Tabs>

### Managed Identity Implementation

<ZoomableImage 
  src="/images/arch/Managed%20Identity-2026-02-23-165751.png" 
  alt="Managed Identity Authentication Architecture" 
  caption="Managed Identity architecture showing credential-less authentication and RBAC role assignments" 
/>

**System-Assigned vs User-Assigned Identity Decision Tree:** The diagram presents the architectural decision framework for selecting between **system-assigned managed identities** (lifecycle tied to a specific resource instance) and **user-assigned managed identities** (independent lifecycle, reusable across resources). For the RMS architecture, the recommendation follows the principle illustrated in the diagram's decision tree: use **system-assigned identities** for single-purpose, ephemeral resources like Azure Functions (where each function's identity permissions are tightly scoped to its specific integration partner, e.g., the `func-mncis-sender` Function has only `ServiceBusSender` role on the `route-commands` topic, nothing more), and use **user-assigned identities** for shared infrastructure components like the RMS API App Service and Routing API App Service that both require identical RBAC permissions to Azure SQL Database, Key Vault, and Service Bus. The diagram shows how a single user-assigned identity named `id-rms-shared-${environment}` can be assigned to multiple App Services, reducing RBAC management overhead from N individual role assignments per service to a single centralized role assignment per target resource.

**Migration Path from Key Vault Stored Credentials to Managed Identities:** The architecture diagram illustrates the phased migration strategy (referenced in Priority Recommendation #1 in Section 10) for eliminating stored credentials currently held in Azure Key Vault. The current state, visualized in the diagram's "Before" pane, shows App Services retrieving connection strings from Key Vault secrets (`AzureWebJobsStorage`, `SqlConnectionString`, `ServiceBusConnectionString`) at startup and passing them to SDK clients like `ServiceBusClient`. This pattern, while encrypted at rest, still involves credentials existing in memory and configuration, creating breach risk if application code is compromised or misconfigured logging exposes connection strings. The target state, shown in the "After" pane, demonstrates managed identity-based authentication where the `ServiceBusClient` constructor receives a `DefaultAzureCredential` token provider instead of a connection string—the Azure SDK automatically obtains an access token from the Azure Instance Metadata Service (IMDS) using the resource's managed identity, authenticates to Service Bus, and handles token refresh without any secrets ever touching application code. The diagram notes that this migration has zero breaking changes to business logic; only the infrastructure-as-code Bicep templates and authentication initialization code require modification.

**RBAC Role Assignments Per Service Shown in Diagram:** The diagram meticulously documents the least-privilege RBAC role assignments required for each microservice when using managed identity authentication, directly addressing the "stored credentials" security gap identified in the Executive Summary. The **RMS Core API** managed identity receives: `Key Vault Crypto User` (to retrieve Always Encrypted column encryption keys without managing secrets), `Azure SQL Database Contributor` (to execute SQL queries via Azure AD authentication), and `Azure Service Bus Data Sender` (to publish case events to the `case-events` topic). The **Routing Service** managed identity receives: `Azure Service Bus Data Receiver` on the `route-commands` subscription and `Azure Service Bus Data Sender` for publishing completion acknowledgments back to status topics. The **Search Indexing Functions** receive read-only roles: `Storage Blob Data Reader` for accessing case documents and `Search Index Data Contributor` for updating Azure AI Search indexes. This granular role decomposition, annotated in the diagram with resource scopes, ensures that even if a service is compromised through an application vulnerability, the attacker gains only narrowly scoped permissions—a compromised Search Function cannot modify SQL data or access encryption keys, limiting blast radius.

**Credential Rotation Elimination Benefits and AAD Token Lifecycle:** The diagram highlights one of Managed Identity's strongest security advantages: **automatic credential rotation** handled entirely by the Azure platform. Unlike connection strings or service principal client secrets that require 90-day rotation workflows (with associated risk of expired credentials causing outages), managed identity access tokens are short-lived (1-hour validity) and automatically refreshed by the Azure SDK's credential chain without application involvement. The diagram illustrates the token acquisition flow: (1) Application code calls `serviceBusClient.SendMessageAsync()`, (2) Azure SDK's `DefaultAzureCredential` detects it's running in an Azure managed environment, (3) SDK queries the Instance Metadata Service (IMDS) endpoint `http://169.254.169.254/metadata/identity/oauth2/token` with the resource's managed identity, (4) IMDS authenticates the caller via Azure fabric identity proof and returns a signed JWT token with `aud` claim matching Service Bus's resource ID, (5) SDK includes the token in the `Authorization: Bearer` header of Service Bus API calls. This entire flow completes in milliseconds. When the token nears expiration (50 minutes into its 60-minute lifetime), the SDK proactively refreshes it using the same IMDS mechanism. This lifecycle management eliminates the operational burden of maintaining credential rotation runbooks and removes the risk of developers hardcoding secrets in application configuration "temporarily" during troubleshooting.

**Zero Secrets in Configuration Advantage and Alignment with Priority Recommendation #1:** As emphasized in the diagram's benefits callout and explicitly referenced in Section 10's Priority Recommendations table (Priority #1: "Implement Managed Identities for all services - 2-3 week effort - Eliminate credential management, reduce breach risk"), the managed identity architecture achieves the security ideal of **zero secrets in application configuration**. The diagram contrasts the current Key Vault-based approach (where `appsettings.json` contains `"KeyVaultUri": "https://kv-rms-prod.vault.usgovcloudapi.net/"`, still a piece of infrastructure knowledge) with the managed identity approach (where configuration contains only `"ServiceBusNamespace": "sb-rms-prod.servicebus.usgovcloudapi.net"`, a non-sensitive routing target). Even if an attacker gains read-access to application configuration files or environment variables, they obtain zero authentication materials. This architecture satisfies CJIS Security Policy 5.10.1's requirement for Advanced Authentication by eliminating static credentials entirely, replacing them with cryptographically signed identity proof that cannot be exfiltrated or replayed outside the Azure environment. The migration to managed identities is the single highest-impact security improvement the RMS architecture can implement, and the diagram provides the technical blueprint for executing this transformation across all 15+ compute and data services in the topology.

### CJIS Compliance Gaps

<Note type="danger" title="Critical CJIS Items Requiring Immediate Action">
- **5.10.1 Encryption In Use** - Evaluate Azure Gov with CJIS Management Agreement OR Confidential Computing
- **5.12 Personnel Security** - Fingerprint-based background checks required for all CJI access
- **5.6.2.2 FIPS 140 Authentication** - Document FIPS 140 compliance of all auth methods
- **CJIS Security Addendum** - Must sign with Microsoft (contact cjis@microsoft.com)
</Note>

| CJIS Policy Area | Status | Priority | Action Required |
|---|---|---|---|
| **5.10.1 Encryption In Use** | MISSING | Critical | Evaluate Azure Gov CJIS Management Agreement |
| **5.12 Personnel Security** | MISSING | Critical | Fingerprint background checks for CJI access |
| **5.6.2.2 FIPS 140 Authenticators** | MISSING | Critical | Document FIPS 140 compliance of all MFA methods |
| **CJIS Security Addendum** | MISSING | Critical | Sign with Microsoft |
| **5.4 Audit Logging** | Partial | High | Extend to all CJI touchpoints, 7+ year retention |
| **5.5 Access Control** | Partial | High | Session lock, account lockout thresholds |
| **5.2 Security Awareness Training** | MISSING | High | Formal training program |
| **5.7/5.10.4 Configuration Management** | MISSING | High | Baseline configs, change control |
| **5.15 System Integrity** | MISSING | High | Vulnerability scanning, integrity monitoring |
| **Customer Lockbox** | MISSING | High | Control Microsoft engineer access |
| **Azure Policy CJIS Initiative** | MISSING | Medium | Deploy built-in CJIS compliance policies |
| **5.1 Information Exchange Agreements** | MISSING | Medium | Formal agreements with external systems |
| **5.3 Incident Response** | Partial | Medium | Formal plan with exercise schedule |

---

## 7. CJIS Data Classification Boundaries

<Mermaid
  chart={`graph TB
    subgraph CJI_Zone["CJIS RESTRICTED ZONE - Background Checks Required"]
      direction TB

      subgraph CJI_Compute["CJI Compute"]
        RMS_CJI["RMS App Service"]
        Route_CJI["Routing Functions"]
        Search_CJI["Search Functions"]
      end

      subgraph CJI_Data["CJI Data Stores"]
        SQL_CJI["Azure SQL Database<br/>TDE + CMK + Always Encrypted<br/>PII: Names, SSN, DOB"]
        Blob_CJI["Blob Storage<br/>Evidence Files<br/>Double Encryption + WORM"]
        SB_CJI["Service Bus Topics<br/>CJI Message Content"]
      end

      subgraph CJI_Security["CJI Security Controls"]
        KV_CJI["Key Vault Managed HSM<br/>FIPS 140-2 Level 3"]
        Audit_CJI["Immutable Audit Logs<br/>7-Year Retention"]
        RBAC_CJI["RBAC: CJI Personnel Only"]
      end

      CJI_Compute --> CJI_Data
      CJI_Data --> CJI_Security
    end

    subgraph Transition["DATA ANONYMIZATION PIPELINE"]
      direction LR
      Extract["Extract"]
      Scrub["Remove PII:<br/>Names, SSN, DOB,<br/>Addresses, Phone"]
      Aggregate["Aggregate +<br/>K-Anonymity"]
      Validate["Re-identification<br/>Risk Check"]
      Load["Load to<br/>Non-CJI Store"]

      Extract --> Scrub
      Scrub --> Aggregate
      Aggregate --> Validate
      Validate --> Load
    end

    subgraph NonCJI_Zone["NON-CJIS ZONE - De-identified Data Only"]
      direction TB

      subgraph NonCJI_AI["AI/ML Services"]
        AISearch["Azure AI Search"]
        DocIntel["Document Intelligence"]
        OpenAI["Azure OpenAI"]
        ML["Azure ML Workspace"]
      end

      subgraph NonCJI_Access["Access Controls"]
        RBAC_Analyst["RBAC: Analysts + Data Scientists<br/>No CJI Access"]
      end

      NonCJI_AI --> NonCJI_Access
    end

    CJI_Zone -->|"One-Way Flow"| Transition
    Transition -->|"De-identified Only"| NonCJI_Zone

    style CJI_Zone fill:#fef2f2,stroke:#dc2626,stroke-width:4px
    style CJI_Data fill:#fecaca,stroke:#b91c1c,stroke-width:2px
    style CJI_Security fill:#fca5a5,stroke:#991b1b,stroke-width:2px
    style Transition fill:#fef9c3,stroke:#ca8a04,stroke-width:3px
    style NonCJI_Zone fill:#ecfdf5,stroke:#059669,stroke-width:4px
    style NonCJI_AI fill:#d1fae5,stroke:#10b981,stroke-width:2px`}
/>

| Data Type | Classification | Encryption | Access Control | Retention |
|-----------|---------------|------------|---------------|-----------|
| **Case records (PII)** | CJI - Restricted | TDE + Always Encrypted | Background-checked personnel | Per agency policy |
| **Evidence files** | CJI - Restricted | Double encryption + WORM | Chain of custody verified | Case lifecycle + 7 years |
| **Audit logs** | CJI - Controlled | Immutable storage | Read-only after write | 7+ years |
| **Integration messages** | CJI - In Transit | TLS 1.2 + SB encryption | Managed Identity auth | TTL-based |
| **Aggregated statistics** | Non-CJI | Standard encryption | Analyst role | 5 years |
| **AI training data** | Non-CJI | Standard encryption | Data scientist role | Model lifecycle |

### CJIS Data Anonymization Pipeline

<ZoomableImage 
  src="/images/arch/CJIS%20Data%20Anonymization-2026-02-23-170447.png" 
  alt="CJIS Data Anonymization Architecture" 
  caption="Data anonymization pipeline showing PII removal, k-anonymity enforcement, and re-identification risk assessment" 
/>

**PII Removal and K-Anonymity Implementation:** The diagram above illustrates the comprehensive data anonymization pipeline that enables safe use of law enforcement data for analytics and AI/ML applications without violating CJIS Security Policy 5.10 requirements. The pipeline implements a multi-stage de-identification process starting with **field-level PII scrubbing** (removing direct identifiers such as names, SSN, date of birth, addresses, and phone numbers), followed by **k-anonymity enforcement** with a threshold of k=5, meaning each anonymized record must be indistinguishable from at least 4 other records when considering quasi-identifiers like age range, geographic region, and case type. This statistical disclosure control technique, visualized in the diagram's aggregation layer, prevents re-identification attacks where an adversary might correlate anonymized data with external datasets to uncover individual identities.

**Re-identification Risk Assessment Methodology:** The pipeline includes a critical validation stage that performs quantitative re-identification risk assessment before any data exits the CJIS restricted zone. As shown in the diagram's "Validate" component, this assessment calculates the probability that a motivated attacker with access to public records databases could successfully link an anonymized record back to a specific individual. The methodology incorporates prosecutor and public defender risk models (assuming adversaries with moderate computational resources and access to county property records, voter registrations, and court dockets) and applies a threshold of < 0.05 re-identification probability per record. Records failing this threshold are either further generalized (e.g., converting specific ages to 5-year age bands) or excluded from the non-CJI dataset entirely. This approach directly implements the principles outlined in NIST SP 800-188 (De-Identifying Government Datasets).

**CJIS Security Policy 5.4 Audit Requirements Compliance:** Every execution of the anonymization pipeline generates immutable audit logs (stored in Azure Monitor Log Analytics with 7-year retention) documenting the transformation applied to each source record, the k-anonymity scores achieved, the re-identification risk scores calculated, and the identity of the data scientist or analyst who initiated the pipeline run. The diagram shows these audit trails flowing to a dedicated "Pipeline Audit Log" store that is subject to quarterly CJIS compliance reviews. This audit capability satisfies Policy 5.4.1's requirement to log all CJI disclosures, treating the anonymization pipeline as a controlled disclosure mechanism. The logs also support forensic investigations if downstream AI systems exhibit biased or problematic behavior—analysts can trace back to the original data selection and transformation logic to identify root causes.

**Data Masking Layers and AI-Ready Architecture Integration:** The architecture implements a defense-in-depth approach to data protection with multiple masking layers visible in the diagram. **Row-level filtering** excludes sensitive case types (e.g., active witness protection cases, juvenile records with court-ordered sealing) before any transformation occurs. **Field-level redaction** removes narrative text fields containing unstructured PII that automated scrubbing might miss (e.g., officer notes mentioning "spoke with John Smith's mother"). **Aggregation-level suppression** prevents small-cell disclosures by suppressing any statistical aggregates derived from fewer than 10 source records. This multi-layered approach, combined with the one-way data flow architecture (CJI → Anonymization → Non-CJI with no reverse path), creates a robust trust boundary. As referenced in Section 9 (AI-Ready Architecture), this anonymization pipeline is the sole mechanism for populating AI/ML training datasets, ensuring that Azure OpenAI services and Azure AI Search indexes in the non-CJI zone never have direct access to raw criminal justice information, eliminating compliance risk while enabling advanced analytics capabilities.

---

## 8. Disaster Recovery Topology

<Mermaid
  chart={`graph TB
    subgraph Primary["PRIMARY REGION: US Gov Virginia"]
      direction TB
      subgraph P_Compute["Compute (Active)"]
        P_APIM["API Management"]
        P_App["App Services"]
        P_Func["Azure Functions"]
      end
      subgraph P_Data["Data (Primary)"]
        P_SQL["SQL Database<br/>(Read/Write)"]
        P_SB["Service Bus<br/>(Primary)"]
        P_Blob["Blob Storage<br/>(RA-GRS)"]
        P_KV["Key Vault"]
      end
    end

    subgraph Secondary["SECONDARY REGION: US Gov Arizona"]
      direction TB
      subgraph S_Compute["Compute (Warm Standby)"]
        S_APIM["API Management"]
        S_App["App Services<br/>(Scaled to Min)"]
        S_Func["Azure Functions<br/>(Scaled to Min)"]
      end
      subgraph S_Data["Data (Replicas)"]
        S_SQL["SQL Database<br/>(Geo-Replica)"]
        S_SB["Service Bus<br/>(Geo-DR Pair)"]
        S_Blob["Blob Storage<br/>(RA-GRS)"]
        S_KV["Key Vault<br/>(Backup)"]
      end
    end

    subgraph Failover["FAILOVER ORCHESTRATION"]
      direction LR
      HealthProbe["Health Probes<br/>Every 30s"]
      FailoverMgr["Traffic Manager<br/>DNS Failover"]
      SQLFailover["SQL Failover Group<br/>Auto 60 min"]
      RunBook["Recovery Runbook"]
    end

    P_SQL -.->|"Async Replication<br/>less than 5 sec lag"| S_SQL
    P_SB -.->|"Geo-DR Pairing"| S_SB
    P_Blob -.->|"RA-GRS"| S_Blob
    P_KV -.->|"Backup/Restore"| S_KV

    HealthProbe -->|"Primary Down"| FailoverMgr
    FailoverMgr -->|"DNS Switch"| S_APIM
    SQLFailover -->|"Promote Replica"| S_SQL
    RunBook -->|"Scale Up"| S_App

    style Primary fill:#ecfdf5,stroke:#059669,stroke-width:4px
    style Secondary fill:#fef3c7,stroke:#d97706,stroke-width:4px
    style Failover fill:#fef2f2,stroke:#dc2626,stroke-width:3px
    style P_SQL fill:#93c5fd,stroke:#1d4ed8,stroke-width:3px
    style S_SQL fill:#fcd34d,stroke:#b45309,stroke-width:3px
    style FailoverMgr fill:#fca5a5,stroke:#b91c1c,stroke-width:3px`}
/>

| Component | RPO (Data Loss) | RTO (Downtime) | Replication | Failover |
|-----------|----------------|----------------|-------------|----------|
| **SQL Database** | < 5 seconds | < 60 minutes | Sync geo-replication | Automatic (failover group) |
| **Service Bus** | Metadata only | < 10 minutes | Geo-DR pairing | Manual alias switch |
| **Blob Storage** | < 15 minutes | < 1 hour | RA-GRS | Manual DNS update |
| **Key Vault** | Manual backup | < 2 hours | Backup/restore | Manual restore |
| **App Services** | N/A (stateless) | < 15 minutes | Warm standby | DNS switch + scale up |
| **Front Door** | N/A | < 5 minutes | Global (built-in) | Automatic health probes |

### Azure SQL Failover Group Configuration

<ZoomableImage 
  src="/images/arch/Azure%20SQL%20Failover%20and-2026-02-23-170534.png" 
  alt="Azure SQL Failover and Geo-Replication Architecture" 
  caption="Azure SQL failover group configuration with active-passive replication and automatic failover" 
/>

**Failover Group Architecture and Active-Passive Configuration:** The diagram illustrates the Azure SQL Database failover group implementation that provides automated geo-redundancy for the RMS transactional database. The architecture deploys an **active-passive** configuration with the primary database in US Gov Virginia handling all read/write traffic and a continuously synchronized secondary replica in US Gov Arizona maintained in hot standby mode. Unlike active-active configurations that distribute writes across regions (which would introduce complex conflict resolution for case management transactions), the active-passive model ensures strong consistency—every committed transaction on the primary is synchronously replicated to the secondary before the commit acknowledgment is returned to the application, guaranteeing zero data loss (RPO = 0) for committed transactions. The diagram shows the failover group listener endpoints that abstract the physical database locations from application connection strings, enabling transparent failover without application code changes.

**Achieving RPO < 5 Seconds with Continuous Data Replication:** The RPO target of "< 5 seconds" documented in the table above represents the maximum age of the most recent transaction that could be lost during an unplanned primary region outage. Azure SQL's active geo-replication technology, visualized in the diagram's replication flow, achieves this aggressive RPO through a **redo log shipping** mechanism that streams transaction log records from the primary to the secondary over the Azure backbone network with typical latency under 1 second. The diagram highlights the asynchronous commit mode used to balance data durability with application performance—transactions commit on the primary after writing to local durable storage, then replication to the secondary occurs in the background. This approach prevents cross-region network latency (approximately 50ms between Virginia and Arizona) from blocking user-facing transactions. In the rare scenario where the primary region fails before recent transactions replicate, the failover process implements **potential data loss detection**, alerting operators to the last successfully replicated transaction timestamp so business users can manually reconcile any lost case updates.

**Manual vs. Automatic Failover Decision Tree:** The diagram presents a critical architectural decision point: configuring the failover group for automatic failover versus manual failover. **Automatic failover** (recommended in the diagram with a 60-minute grace period) triggers when the primary database becomes unreachable for longer than the configured detection timeout. Azure's health probe system continuously monitors database availability every 30 seconds, and after 4 consecutive failures (2 minutes of downtime), begins a 60-minute waiting period to distinguish between transient network issues and genuine regional outages. If the primary remains unavailable after this grace period, the system automatically promotes the secondary to primary, updates DNS records for the failover group listener, and sends Azure Monitor alerts to the operations team. **Manual failover**, alternatively, requires an authorized operator to explicitly invoke the failover command via Azure Portal, CLI, or automation runbook. The diagram recommends automatic failover for production RMS environments to minimize downtime during after-hours or weekend regional outages, but notes that manual failover provides greater control for planned maintenance scenarios where operators want to validate application state before cutting over.

**Connection String Handling and Read/Write Endpoint Strategy:** Application services (RMS API, Routing Service, Search Service) use the failover group listener connection strings shown in the diagram rather than connecting directly to specific database servers. The architecture provisions two listener endpoints: a **read/write listener** (e.g., `rms-failover-group.database.usgovcloudapi.net`) that always points to the current primary database and automatically updates during failover, and a **read-only listener** (e.g., `rms-failover-group.secondary.database.usgovcloudapi.net`) that explicitly targets the secondary replica for offloading read-intensive reporting queries. The diagram illustrates how the RMS Core API uses the read/write listener for transactional case management operations, while the reporting dashboard and analytics queries connect to the read-only endpoint to prevent report generation from impacting officer productivity. During failover, the read/write listener's DNS record is updated within 5 minutes (contributing to the 60-minute total RTO), and application connection pools automatically reconnect to the newly promoted primary. This listener-based approach eliminates the need for application code to implement region-aware routing logic or maintain multiple connection string configurations per environment.

### RPO/RTO Objectives and Recovery Prioritization

<ZoomableImage 
  src="/images/arch/RPO_RTO%20Objectives%20and-2026-02-23-170624.png" 
  alt="RPO/RTO Objectives and Tier-Based Recovery" 
  caption="Recovery objectives with tier-based prioritization for business continuity" 
/>

**Business Process Mapping to Recovery Objectives:** The diagram illustrates the comprehensive Recovery Point Objective (RPO) and Recovery Time Objective (RTO) targets mapped to critical RMS business processes, derived from stakeholder interviews with patrol officers, detectives, dispatch supervisors, and IT operations staff. The architecture implements a **tier-based recovery prioritization model** (Tier 0: Critical, Tier 1: Important, Tier 2: Normal) that acknowledges not all system components require the same aggressive recovery targets. Tier 0 components—the RMS Core API for active case management, Azure SQL Database for transactional case records, and Azure Front Door for officer access—have the most stringent targets (RPO < 5 seconds, RTO < 60 minutes) because their unavailability directly prevents officers from documenting incidents in the field or accessing case histories during traffic stops. Tier 1 components like the Routing Service for external integrations and Service Bus message queues have relaxed targets (RPO < 15 minutes, RTO < 2 hours) since temporary delays in submitting cases to MNCIS or NIBRS do not immediately impact field operations. Tier 2 components such as the reporting dashboard and analytics queries tolerate even longer outages (RTO < 8 hours) as they support administrative workflows rather than real-time public safety operations.

**Acceptable Data Loss Windows and Operational Reasoning:** The RPO targets annotated in the diagram represent the maximum amount of recent data the organization is willing to lose during a disaster scenario, informed by cost-benefit analysis of replication technology investments versus business impact. For the RMS transactional database, the RPO < 5 seconds target means that in a catastrophic primary region failure, officers might need to re-enter up to 5 seconds of case updates manually—potentially affecting 1-2 active case submissions during peak activity hours. This narrow data loss window justifies the investment in Azure SQL active geo-replication (which incurs egress costs for cross-region data transfer and requires doubling database compute/storage costs for the secondary replica). For Service Bus message queues with RPO < 15 minutes, the organization accepts that messages published in the 15 minutes before a disaster might be lost, requiring manual resubmission of affected case routing requests identified through Application Insights distributed tracing logs. This decision balances the cost of Service Bus Premium geo-disaster recovery pairing (~$2,500/month) against the operational impact of occasionally reprocessing a small batch of integration messages.

**Downtime Cost Analysis and Investment Justification:** The RTO targets in the diagram directly correlate to quantified downtime costs that justify disaster recovery infrastructure investments. Internal analysis (detailed in the "DR Cost Model" section of the operational playbooks) estimates that complete RMS unavailability costs approximately **$15,000 per hour** in officer productivity loss (officers reverting to paper forms, delayed case filings causing court continuances, inability to access warrant information during traffic stops). The Tier 0 RTO < 60 minutes target therefore limits worst-case financial exposure to $15,000 for a single regional outage, compared to potential 8-12 hour recovery times without geo-redundancy that could exceed $180,000 in impact. The diagram shows how this cost model drove the decision to implement automatic SQL failover, warm standby App Service deployments in the secondary region, and Azure Front Door's built-in multi-region routing—collectively adding approximately $8,000/month in infrastructure costs but protecting against potentially catastrophic business disruptions.

**DR Testing Schedule and Validation Procedures:** The diagram references a rigorous DR testing program designed to validate that actual recovery times match the documented RTO targets. The architecture mandates **quarterly failover drills** (scheduled 90 days apart, documented in Azure DevOps work items) where the operations team deliberately fails over to the secondary region during a planned maintenance window, measures time-to-recovery for each component, and documents discrepancies. The diagram illustrates the test procedure: (1) Announce test window to stakeholders, (2) Trigger SQL failover group promotion, (3) Update App Service deployment slots to secondary region, (4) Validate application functionality with smoke tests, (5) Measure and record actual RTO/RPO achieved, (6) Conduct post-mortem to identify process improvements. This testing discipline ensures that DR runbooks remain accurate as the architecture evolves and that operations staff maintain proficiency in failover procedures. Test results are aggregated in the **DR Readiness Dashboard** (visible in the diagram) that tracks failover success rate, average RTO variance from target, and runbook accuracy metrics, providing executive leadership with confidence in disaster recovery preparedness.

### Reliability Recommendations

| Item | Current State | Recommendation | Effort |
|------|--------------|----------------|--------|
| **DR Strategy** | No documented plan | Define RTO/RPO, create runbooks | 1 week |
| **SQL Geo-Replication** | Single-region | Enable geo-replication to secondary | 2 weeks |
| **Service Bus Geo-DR** | Standard tier | Upgrade to Premium for geo-DR | 2 weeks |
| **Health Endpoints** | Basic monitoring | Comprehensive health checks per service | 1 week |
| **Retry Policies** | Limited | Exponential backoff + circuit breakers | 2 weeks |
| **DR Testing** | Never tested | Quarterly failover drills | Ongoing |

---

## 9. AI-Ready Architecture (Future State)

<Mermaid
  chart={`graph TB
    subgraph CJI_Source["CJI DATA SOURCES"]
      direction LR
      SQL_Source["Azure SQL<br/>Case Records"]
      SB_Events["Service Bus<br/>Events"]
      Blob_Docs["Blob Storage<br/>Documents"]
    end

    subgraph Pipeline["ANONYMIZATION PIPELINE"]
      direction TB

      subgraph Transform["De-Identify"]
        RemovePII["Remove PII:<br/>Names, SSN, DOB,<br/>Addresses, Phone"]
        SyntheticID["Generate<br/>Synthetic IDs"]
        KAnon["K-Anonymity<br/>Check k=5"]
      end

      subgraph Validate["Validate"]
        ReID_Risk["Re-identification<br/>Risk Score"]
        AuditTrail["Pipeline<br/>Audit Log"]
      end
    end

    subgraph AI_Services["AI/ML SERVICES (Non-CJI Zone)"]
      direction TB

      subgraph Search_AI["Search"]
        SemanticSearch["Azure AI Search<br/>Semantic + Vector"]
      end

      subgraph Doc_AI["Documents"]
        DocIntel["Document Intelligence<br/>OCR + Form Extraction"]
      end

      subgraph GenAI["Generative AI"]
        OpenAI_Svc["Azure OpenAI GPT-4"]
        RAG["RAG Pattern<br/>AI Search + OpenAI"]
        ContentFilter["Content Safety<br/>Filters"]
        HumanReview["Human-in-the-Loop<br/>Review Required"]
      end
    end

    subgraph Outputs["AI-POWERED OUTPUTS"]
      direction LR
      SearchResults["Enhanced Search"]
      FormExtraction["Form Data Extraction"]
      WritingAssist["Report Writing Assist"]
    end

    SQL_Source --> RemovePII
    SB_Events --> RemovePII
    RemovePII --> SyntheticID
    SyntheticID --> KAnon
    KAnon --> ReID_Risk
    ReID_Risk --> AuditTrail

    AuditTrail --> SemanticSearch
    AuditTrail --> DocIntel
    Blob_Docs --> DocIntel
    SemanticSearch --> RAG
    RAG --> OpenAI_Svc
    OpenAI_Svc --> ContentFilter
    ContentFilter --> HumanReview

    SemanticSearch --> SearchResults
    DocIntel --> FormExtraction
    HumanReview --> WritingAssist

    style CJI_Source fill:#fef2f2,stroke:#dc2626,stroke-width:3px
    style Pipeline fill:#fef9c3,stroke:#ca8a04,stroke-width:3px
    style AI_Services fill:#ecfdf5,stroke:#059669,stroke-width:3px
    style Outputs fill:#dbeafe,stroke:#2563eb,stroke-width:2px
    style GenAI fill:#f3e8ff,stroke:#7c3aed,stroke-width:2px`}
/>

<Note type="warning" title="CJIS Compliance for AI">
All AI models trained only on de-identified data. No CJI used in external AI services unless CJIS-compliant. All AI outputs require human review before use in CJIS systems. Document all AI decision-making per NIST AI Risk Management Framework.
</Note>

| Use Case | Complexity | CJIS Impact | Business Value |
|----------|-----------|-------------|---------------|
| **Enhanced Search** (AI Search) | Medium | None (non-CJI zone) | High - Faster case lookup |
| **Form Extraction** (Document Intelligence) | Medium | Document-level review | High - Reduce manual entry |
| **Report Writing Assist** (OpenAI) | High | Human review required | High - Officer productivity |
| **Case Duration Prediction** (ML) | High | None (aggregated data) | Medium - Resource planning |

---

## 10. Implementation Roadmap

<Mermaid
  chart={`gantt
    title CJN Dakota RMS - Implementation Roadmap
    dateFormat YYYY-MM-DD
    axisFormat %b %Y

    section Phase 0: Planning
    Stakeholder Review and Approval       :done, p0a, 2026-02-20, 14d
    Answer Unasked Questions               :active, p0b, 2026-03-01, 14d
    Deep-Dive Sessions                     :p0c, 2026-03-10, 14d

    section Phase 1: Security
    Managed Identities                     :crit, p1a, 2026-03-24, 21d
    SQL TDE with CMK                       :crit, p1b, 2026-03-24, 14d
    Always Encrypted for PII               :p1c, 2026-04-07, 21d
    Private Link for PaaS                  :p1d, 2026-04-07, 28d
    FIPS 140 Auth Validation               :p1f, 2026-04-14, 14d

    section Phase 2: Reliability
    RPO/RTO Targets                        :p2a, 2026-05-04, 7d
    SQL Geo-Replication                    :crit, p2b, 2026-05-11, 14d
    Service Bus Geo-DR                     :p2c, 2026-05-25, 14d
    Health Endpoints                       :p2d, 2026-05-11, 7d
    DR Runbooks and Testing                :p2e, 2026-06-08, 14d

    section Phase 3: Operations
    Distributed Tracing                    :p3a, 2026-06-15, 7d
    Azure Monitor Workbooks                :p3b, 2026-06-22, 14d
    Microsoft Sentinel Setup               :p3c, 2026-06-22, 28d
    Integration Health Dashboard           :p3e, 2026-07-13, 14d

    section Phase 4: Performance
    Azure Cache for Redis                  :p4a, 2026-07-27, 14d
    Auto-Scaling Configuration             :p4b, 2026-07-27, 14d
    API Versioning Strategy                :p4c, 2026-08-10, 14d
    Load Testing                           :p4d, 2026-08-24, 14d

    section Phase 5: AI Enablement
    Data Segregation Architecture          :p6a, 2026-09-21, 21d
    Anonymization Pipeline                 :p6b, 2026-10-12, 28d
    Azure AI Search POC                    :p6c, 2026-11-09, 42d
    Document Intelligence Pilot            :p6d, 2026-12-21, 28d`}
/>

| Phase | Focus | Key Deliverables | Dependencies |
|-------|-------|-----------------|-------------|
| **0** | Planning | Approved roadmap, answered questions, deep-dive sessions | Stakeholder alignment |
| **1** | Security | Managed Identities, TDE/CMK, Private Link, CJIS addendum | Phase 0 complete |
| **2** | Reliability | Geo-replication, DR plan, health monitoring, tested failover | Phase 1 foundations |
| **3** | Operations | Distributed tracing, Sentinel SIEM, dashboards, alerting | Phase 2 health endpoints |
| **4** | Performance | Redis cache, auto-scaling, load testing, API versioning | Phase 3 monitoring |
| **5** | AI | Data segregation, anonymization pipeline, AI Search POC | Phase 1 + Phase 3 |

---

## Priority Recommendations

### Immediate Actions (Next 30 Days)

| # | Recommendation | Pillar | Effort | Business Impact |
|---|----------------|--------|--------|----------------|
| 1 | Implement Managed Identities for all services | Security | 2-3 weeks | Eliminate credential management, reduce breach risk |
| 2 | Enable SQL TDE with Customer-Managed Keys | Security | 2 weeks | CJIS compliance, enhanced key control |
| 3 | Configure Application Insights distributed tracing | Operations | 1 week | End-to-end visibility, faster troubleshooting |
| 4 | Implement health endpoints and monitoring | Reliability | 1 week | Proactive issue detection, prevent outages |
| 5 | Enable SQL Database geo-replication | Reliability | 2 weeks | Multi-region redundancy, disaster recovery |
| 6 | Document DR procedures and RPO/RTO | Reliability | 1 week | Clear recovery processes, compliance |

### Short-term (Next 90 Days)

| # | Recommendation | Pillar | Effort | Business Impact |
|---|----------------|--------|--------|----------------|
| 7 | Implement Private Link for all PaaS services | Security | 3-4 weeks | Network isolation, eliminate public exposure |
| 8 | Configure Azure Cache for Redis | Performance | 2 weeks | Faster response times, better user experience |
| 9 | Implement API versioning strategy | Operations | 2 weeks | Safe evolution, backward compatibility |
| 10 | Set up Microsoft Sentinel for SIEM | Security | 3-4 weeks | Advanced threat detection, automated response |
| 11 | Configure auto-scaling rules | Performance | 2 weeks | Handle traffic spikes, maintain performance |
| 12 | Implement storage lifecycle management | Operations | 1 week | Efficient data management, compliance |

### Long-term (90+ Days)

| # | Recommendation | Pillar | Effort | Business Impact |
|---|----------------|--------|--------|----------------|
| 13 | Evaluate AI use cases (non-CJIS data) | Innovation | 6-8 weeks | Enhanced analytics, predictive capabilities |
| 14 | Implement Event Sourcing for audit trail | Reliability | 8-12 weeks | Complete audit history, forensic analysis |
| 15 | Consider CQRS for read/write separation | Performance | 10-12 weeks | Optimized queries, scalable architecture |
| 16 | Migrate to Premium Service Bus for geo-DR | Reliability | 2 weeks | Geographic redundancy, business continuity |

Visit the [Priority Matrix](/docs/priority-matrix) to track implementation progress interactively.

---

## Appendix

### A. RBAC Role Matrix

<ZoomableImage 
  src="/images/arch/User%20Role%20Access%20Management-2026-02-23-171421.png" 
  alt="User Role Access Management" 
  caption="RBAC architecture showing authentication flow, Conditional Access policies, and role-based permissions" 
/>

#### Visual RBAC Flow and Access Control Architecture

The User Role Access Management diagram provides a comprehensive visualization of the role-based access control (RBAC) flow across all system layers, from Azure Active Directory (Entra ID) authentication through API Management authorization, application-level permission enforcement, and database-level row security. This end-to-end RBAC architecture implements defense-in-depth access control aligned with **CJIS Security Policy 5.6 (Personnel Security)** requirements, ensuring that access privileges are validated at every trust boundary and that no single layer's compromise can bypass authorization checks. The diagram directly maps to the RBAC Role Matrix table below, illustrating how organizational roles (Patrol Officer, Detective, Supervisor, Admin) translate into technical permissions across Azure infrastructure, application code, and data storage layers.

**RBAC Flow Across System Layers:** The diagram traces the complete authentication and authorization path for a typical user request. **Layer 1 (Azure AD / Entra ID)**: Officers authenticate using their organizational credentials (`officer.smith@dakotacounty.gov`), with Multi-Factor Authentication (MFA) enforcement via Conditional Access policies that require Authenticator app approval and compliant device check (Azure AD Join or Intune MDM enrollment). Upon successful authentication, Entra ID issues a **JWT access token** containing user claims: `oid` (unique user object ID), `roles` (array of assigned application roles: `["PatrolOfficer", "CJIUser"]`), `groups` (Azure AD security group memberships: `["RMS-DakotaSheriff-Users"]`), and `tid` (tenant ID for multi-tenant isolation, referencing Section 1.6). This JWT token serves as the user's identity credential for all subsequent API calls. **Layer 2 (API Management)**: Azure APIM validates the JWT token signature against Entra ID's public signing keys (JWKS endpoint), verifies token expiration and issuer claims, and extracts the `roles` claim for initial authorization checks. APIM policies implement coarse-grained authorization: `<validate-jwt>` policy blocks requests without valid tokens, `<check-header>` policy requires TLS client certificates for API-to-API calls, and `<rate-limit-by-key>` policy enforces per-role quotas (patrol officers limited to 60 requests/minute, detectives allowed 200 requests/minute to support high-volume investigative queries). **Layer 3 (Application Tier)**: RMS App Service receives the validated JWT and performs fine-grained authorization using ASP.NET Core's `[Authorize(Roles = "Detective")]` attributes on controller actions. The application code inspects claims to enforce business rules: patrol officers can read only their own cases (`if (currentUser.OfficerId != case.CreatedByOfficerId) throw new ForbiddenException()`), while detectives can read all cases within their department (`if (currentUser.Department != case.Department) throw new ForbiddenException()`). **Layer 4 (Database Tier)**: Azure SQL Database enforces row-level security (RLS) policies that automatically filter query results based on the `Managed Identity` or `SQL User` principal executing the query. For tenant isolation, RLS appends `WHERE TenantId = SESSION_CONTEXT('TenantId')` to all SELECT statements, with the application setting session context after authentication. This layered access control ensures that even SQL injection vulnerabilities cannot bypass authorization—the database engine enforces access restrictions independently of application code.

**Conditional Access Policy Flow and Enforcement Points:** The diagram details the Conditional Access policy evaluation chain that occurs during Entra ID authentication, implementing Zero Trust principles where every authentication request is evaluated against dynamic risk signals. Conditional Access policies are configured with **Grant Controls** (Require MFA, Require compliant device, Require hybrid Azure AD joined device) and **Session Controls** (Application enforced restrictions, Conditional Access App Control with Microsoft Defender for Cloud Apps monitoring). The evaluation flow follows a decision tree: (1) user initiates authentication → (2) Entra ID checks user's location (is the request originating from known Dakota County IP ranges or via VPN?) → (3) device compliance check (is the device Intune-managed with required security patches?) → (4) user risk score (has this account been observed in credential leak databases or exhibiting anomalous behavior patterns?) → (5) sign-in risk score (is this login attempt from an impossible travel scenario, e.g., Minneapolis 10 minutes ago, now Moscow?). If any risk signal exceeds thresholds (user risk: High, sign-in risk: Medium), Conditional Access **blocks** the authentication and requires security team remediation. If risk signals are acceptable, Conditional Access grants a session token with **token lifetime restrictions**: standard users receive 8-hour tokens requiring re-authentication each shift, while external partner accounts receive 1-hour tokens with reauthentication prompts. These session controls are visualized in the diagram as decision gates between Entra ID and the application layer, emphasizing that authentication is continuously evaluated rather than a one-time check.

**Privileged Identity Management (PIM) Just-in-Time Access Workflow:** The diagram prominently features the **Azure PIM workflow** for administrative access to production resources, implementing time-bound, approval-gated privilege elevation. Standard operational procedures dictate that database administrators and infrastructure engineers hold **Eligible Role Assignments** (Contributor on RMS-Core-RG, Owner on RMS-Shared-RG) but do not have **Active Role Assignments** by default. When an engineer requires elevated access for incident response, they initiate a **PIM Activation Request** via the Azure Portal or PowerShell module, specifying: (1) requested role (Contributor on RMS-Core-RG), (2) duration (maximum 4 hours), (3) business justification ("investigating P1 incident #12345 - API Gateway 502 errors"), and (4) optionally attaching an approved Change Request ticket ID. The PIM workflow routes the request to designated **Approvers** (two infrastructure architects must approve), who receive notifications via Microsoft Teams and email with full context (requester identity, requested scope, justification, approval deadline). If approved within 15 minutes, PIM grants the requested role for the specified duration and logs the activation to Azure Activity Log and Microsoft Sentinel. After 4 hours, the role assignment **automatically expires**, requiring reactivation if additional access is needed. This JIT access model—critical for CJIS compliance—ensures that administrative privileges are time-limited, auditable, and require peer approval, drastically reducing the attack surface compared to permanent global administrator assignments. The diagram maps PIM activation to the **Admin** row in the RBAC Role Matrix below, showing that full access is gated by "(via PIM)" annotations.

**Break-Glass Account Strategy for Emergency Access:** The architecture implements a **break-glass account** strategy for emergency scenarios where normal authentication mechanisms fail (Entra ID outage, MFA provider unavailable, Conditional Access policy misconfiguration locking out all administrators). Two break-glass accounts (`bg-admin-001@dakotacounty.onmicrosoft.com`, `bg-admin-002@dakotacounty.onmicrosoft.com`) are provisioned with **permanent Global Administrator** privileges and **excluded from all Conditional Access policies**. These accounts are secured with 64-character randomly-generated passwords stored in a physical safe (with dual-access control requiring two executives) and monitored continuously—any authentication event using break-glass accounts triggers **immediate P0 alerts** to the security team and executive leadership. The accounts are tested quarterly during disaster recovery drills to validate they remain functional and passwords are rotated annually with witness verification. The diagram shows break-glass accounts as a separate authentication path bypassing normal RBAC flows, with a direct line to resource access layers marked "EMERGENCY ONLY - FULLY AUDITED". This contingency access pattern balances business continuity (preventing lockout scenarios) with security rigor (extensive logging, physical security controls, quarterly accountability reviews).

**CJIS Security Policy 5.6 Personnel Security Alignment:** The RBAC architecture directly implements CJIS requirements for personnel security: **5.6.1.1 (Personnel Screening)** verified through background check flags in Entra ID user profiles (`extensionAttribute1: "BackgroundCheckCompleted-2025-06-15"`), **5.6.2.1 (Training)** enforced via mandatory annual CJIS training completion records that gate access (Conditional Access policy: `if (user.trainingExpiry < today) deny()`), and **5.6.3.1 (Separation of Duties)** implemented via distinct roles with non-overlapping permissions (patrol officers cannot approve supervisor actions, analysts cannot access PII fields). The diagram cross-references these CJIS controls, annotating each access layer with the applicable policy section. Audit logs—captured at Entra ID sign-ins, APIM gateway requests, application-level authorization decisions, and SQL Database row access—provide **end-to-end forensic trails** for compliance audits, enabling investigators to reconstruct complete access histories: "which users accessed Case #12345's evidence photos during the investigation period?". These logs are exported to Azure Log Analytics, retained for 7 years per CJIS requirements, and protected with immutable storage (Azure Blob WORM policy) to prevent tampering.

**Entra B2B Guest User Access Patterns:** The architecture supports **external partner access** via Entra ID B2B (business-to-business) collaboration, enabling prosecutors from Hennepin County or state investigators from BCA to access shared cases without creating duplicate user accounts. Guest users authenticate with their home organization's credentials (e.g., `@hennepin.us` Entra ID tenant) and receive **JIT guest invitations** scoped to specific cases or evidence items. The RBAC diagram illustrates the guest user flow: external user authenticates to Hennepin County Entra ID → cross-tenant token minted by Dakota County Entra ID (home tenant provides identity, resource tenant provides authorization) → guest user receives restricted permissions (Read-Only access to assigned cases, no download/print rights, watermarked evidence previews). Guest access sessions are limited to **2-hour lifetime** with explicit reauthentication after session expiry, and all guest user activities generate **high-severity audit events** logged to Microsoft Sentinel for security monitoring. The diagram maps guest users to the **External System** row in the RBAC Role Matrix, specifying "API Scoped" and "Per Agreement" constraints. This B2B integration pattern—replacing insecure email attachments or FTP file shares—provides secure, auditable, time-limited collaboration with external law enforcement agencies while maintaining tenant isolation (external users never receive native Entra ID accounts in Dakota County's directory).

#### RBAC Role Matrix Table

| Role | Cases | Evidence | Admin | Reports | Integration | CJIS Level |
|------|-------|----------|-------|---------|-------------|------------|
| **Patrol Officer** | Create/Read Own | Upload | None | Own Dept | None | CJI Access |
| **Detective** | Read/Update All in Dept | Full Access | None | Dept-wide | None | CJI Access |
| **Supervisor** | Full Dept Access | Full Access | User Mgmt | All Reports | Config | CJI Access |
| **Admin** | All (via PIM) | All (via PIM) | Full | All | Full | CJI + Background Check |
| **Analyst** | Read (de-identified) | None | None | Aggregated | None | Non-CJI |
| **External System** | API Scoped | None | None | None | Specific API | Per Agreement |

### B. CI/CD Pipeline Stages

<ZoomableImage 
  src="/images/arch/Azure%20DevOps%20CI_CD%20Pipeline-2026-02-23-170737.png" 
  alt="Azure DevOps CI/CD Pipeline Architecture" 
  caption="CI/CD pipeline showing build stages, approval gates, and deployment workflow" 
/>

#### Deployment Pipeline Architecture and Infrastructure as Code

The Azure DevOps CI/CD pipeline diagram illustrates the comprehensive continuous integration and continuous deployment workflow that automates the delivery of application code and infrastructure changes to the CJN Dakota RMS environment. This pipeline architecture implements best practices for security scanning, approval gates, and automated rollback mechanisms to ensure production deployments meet CJIS compliance requirements while enabling rapid iteration velocity. The pipeline integrates with the Bicep Infrastructure-as-Code (IaC) toolchain referenced in Section 1's **Technology Stack** table ("IaC: Bicep + Azure DevOps"), providing a declarative, version-controlled approach to managing all Azure resources.

**Branch Strategy and Merge Policies:** The diagram illustrates the Git branching model: developers work in **feature branches** (`feature/case-search-enhancement`), which merge to the **develop branch** via pull requests (PRs) that trigger automated CI builds, unit tests, and Static Application Security Testing (SAST) scans. Once validated in the develop environment, changes are promoted to the **main branch** through a gated PR requiring two code reviewer approvals and passing integration tests. The main branch automatically deploys to the **production environment** after manual approval from the release manager. Azure Repos enforces branch policies: direct commits to main or develop are blocked, PR authors cannot approve their own PRs, and all PRs must include linked Azure DevOps work items (user stories, bugs) for traceability. This branching strategy—based on GitFlow—prevents untested code from reaching production and maintains a clean audit trail for CJIS compliance ("who deployed what, when, and why").

**Bicep IaC Deployment Pipeline Stages:** The infrastructure deployment pipeline (triggered by commits to the `infra/` directory) executes a multi-stage workflow visualized in the diagram: (1) **Bicep Compilation**: validate `.bicep` files with `az bicep build` to catch syntax errors, (2) **What-If Analysis**: run `az deployment group what-if` to preview infrastructure changes without applying them (e.g., "This will create 3 new resources, modify 1 existing resource, and delete 0 resources"), (3) **Peer Review Gate**: block deployment until two infrastructure engineers approve the what-if diff, (4) **Incremental Deployment**: execute `az deployment group create --mode Incremental` to apply only the delta changes, preserving existing resources not defined in Bicep templates, (5) **Smoke Tests**: validate critical resource configurations (e.g., verify App Service has VNet integration enabled, confirm Key Vault has purge protection), and (6) **Deployment Report**: publish a summary to the Azure DevOps pipeline run with links to Azure Portal resource groups. This IaC approach eliminates manual Azure Portal clicking, provides disaster recovery capability (re-deploy entire infrastructure from Git history), and enables infrastructure drift detection (comparing actual Azure state to Bicep templates via what-if analysis).

**Approval Gates and Manual Intervention Points:** The diagram highlights three critical manual approval stages: **Stage Environment Deployment** (requires QA team sign-off after smoke tests pass), **Production Deployment** (requires release manager approval with mandatory 4-hour delay for business hours deployments), and **Infrastructure Changes** (requires dual approval from infrastructure architects for security-sensitive changes like NSG rule modifications or Private Link configurations). Each approval gate integrates with Azure DevOps Approvals API, which sends notifications to Microsoft Teams channels and enforces timeout policies (approvals expire after 24 hours, blocking stale deployments). For emergency hotfixes, the pipeline supports a "fast-track" path that bypasses the stage environment and deploys directly to production, but requires dual approval and generates a high-severity audit log entry for post-incident review. These approval gates balance deployment velocity (multiple deployments per day to dev/stage) with production stability (zero unplanned downtime in 6 months, per customer interviews).

**Rollback Procedure and Version Tagging:** The pipeline implements automated rollback capability using Azure App Service deployment slots. Each production deployment first publishes to a **staging slot**, performs health checks (HTTP 200 responses, Application Insights availability tests, synthetic transaction validation), and then swaps the staging slot to production using `az webapp deployment slot swap`. If post-deployment health checks fail—indicated by error rate exceeding 1% within 5 minutes or Application Insights anomaly detection triggering—the pipeline automatically swaps back to the previous version within 60 seconds, restoring service. The diagram shows the version tagging strategy: every successful deployment tags the Git commit with the semantic version (`v2.3.15`) and deployment timestamp, creating an immutable release history. These tags enable point-in-time rollback: operators can redeploy any previous version by triggering the pipeline with a specific Git tag as the source. For infrastructure changes, Bicep templates are tagged similarly (`infra-v1.2.0`), and Azure Policy assignments enforce tagging requirements on all resources ("all resources must have tags: Environment, Owner, CostCenter, Version").

**Infrastructure Drift Detection and Security Scanning Integration:** The pipeline includes a nightly scheduled job (visualized in the diagram as a separate pipeline trigger) that runs `az deployment group what-if` against production resource groups to detect configuration drift—manual changes made outside the CI/CD pipeline (e.g., an engineer adjusting NSG rules via Azure Portal). If drift is detected, the pipeline creates a high-priority Azure DevOps work item with a detailed diff and alerts the infrastructure team. This drift detection—combined with Azure Resource Graph queries that identify untagged or non-compliant resources—ensures the production environment matches the declarative Bicep templates. The diagram also highlights **Microsoft Defender for DevOps** integration, which scans Infrastructure-as-Code templates for security misconfigurations (e.g., storage accounts without encryption, Key Vaults with public network access enabled) and blocks deployments with critical findings. Defender for DevOps extends to application code scanning, integrating SAST tools (Checkmarx, SonarQube) and Software Composition Analysis (SCA) tools (WhiteSource, Snyk) to identify vulnerable dependencies (CVEs in NuGet packages) and code quality issues (SQL injection risks, hardcoded secrets). These security scans run in parallel during the CI build stage, failing the build if critical vulnerabilities are detected, thus preventing vulnerable code from reaching production.

| Stage | Trigger | Gate | Failure Action |
|-------|---------|------|---------------|
| **CI Build** | PR or merge to main | Automated (compilation) | Block merge |
| **Unit Tests** | CI pipeline | 90%+ coverage required | Block merge |
| **SAST/Dep Scan** | CI pipeline | No critical vulnerabilities | Block merge |
| **Dev Deploy** | Merge to main | Smoke tests pass | Alert team |
| **Stage Deploy** | Manual promote | QA approval required | Block promotion |
| **Prod Deploy** | Manual approval | Error rate < 1% | Auto-rollback |
| **Infra Deploy** | PR to infra/ branch | Peer review + what-if | Block apply |

### C. Monitoring Alert Thresholds

| Metric | Warning | Critical | Response |
|--------|---------|----------|----------|
| **API Response Time** | > 2 seconds | > 5 seconds | Scale out App Service |
| **Error Rate** | > 1% | > 5% | Page on-call, initiate rollback |
| **CPU Utilization** | > 70% | > 90% | Auto-scale (configured) |
| **DLQ Message Count** | > 5 | > 20 | Investigate failed integrations |
| **SQL DTU Usage** | > 70% | > 90% | Scale database tier |
| **Failed Authentications** | > 5/min | > 20/min | Sentinel auto-block IP |
| **Service Bus Queue Depth** | > 1000 | > 5000 | Scale consumers |
| **Certificate Expiry** | < 30 days | < 7 days | Auto-renew or page team |

### D. Data Lifecycle Storage Tiers

| Phase | SQL Tier | Blob Tier | Cache | Transition |
|-------|----------|-----------|-------|------------|
| **Active** (0-90 days) | General Purpose | Hot | Redis cached | Manual |
| **Investigation** (90d - 2y) | General Purpose | Cool (auto at 90d) | Evicted | Lifecycle policy |
| **Closed** (2-7 years) | Partitioned archive | Archive (auto at 730d) | None | Lifecycle policy |
| **Disposal** (7+ years) | Purge (legal hold check) | Purge (WORM check) | N/A | Manual + approval |

### E. Pillar Maturity Assessment

| Pillar | Sub-Area | Current | Target | Gap | Priority |
|--------|----------|---------|--------|-----|----------|
| **Security** | Identity Management | 7/10 | 9/10 | 2 | High |
| **Security** | Data Protection | 6/10 | 9/10 | 3 | Critical |
| **Security** | Network Security | 7/10 | 9/10 | 2 | High |
| **Security** | CJIS Compliance | 6/10 | 9/10 | 3 | Critical |
| **Reliability** | Disaster Recovery | 4/10 | 9/10 | 5 | Critical |
| **Reliability** | Health Monitoring | 5/10 | 8/10 | 3 | High |
| **Reliability** | Resilience Patterns | 6/10 | 8/10 | 2 | Medium |
| **Operations** | Observability | 5/10 | 8/10 | 3 | High |
| **Operations** | CI/CD Maturity | 7/10 | 9/10 | 2 | Medium |
| **Operations** | Incident Response | 4/10 | 8/10 | 4 | High |
| **Performance** | Caching Strategy | 3/10 | 8/10 | 5 | High |
| **Performance** | Auto-Scaling | 4/10 | 8/10 | 4 | Medium |
| **Performance** | Database Optimization | 6/10 | 8/10 | 2 | Medium |

### F. Azure Verified Modules (AVM)

| Recommendation | AVM Module | Registry Path |
|----------------|------------|---------------|
| Managed Identities | User Assigned Identity | `br/public:avm/res/managed-identity/user-assigned-identity` |
| SQL Database + TDE | SQL Server | `br/public:avm/res/sql/server` |
| Private Endpoints | Private Endpoint | `br/public:avm/res/network/private-endpoint` |
| Key Vault (HSM) | Key Vault | `br/public:avm/res/key-vault/vault` |
| Service Bus | Service Bus Namespace | `br/public:avm/res/service-bus/namespace` |
| Azure Functions | Web/Sites | `br/public:avm/res/web/site` |
| API Management | APIM Service | `br/public:avm/res/api-management/service` |
| Application Insights | Insights Components | `br/public:avm/res/insights/component` |
| Redis Cache | Cache for Redis | `br/public:avm/res/cache/redis` |
| Front Door | CDN Profile | `br/public:avm/res/cdn/profile` |

### Resources

- [Azure Well-Architected Framework](https://learn.microsoft.com/azure/architecture/framework/)
- [CJIS Compliance in Azure](https://learn.microsoft.com/azure/compliance/offerings/offering-cjis)
- [Azure Security Baseline](https://learn.microsoft.com/security/benchmark/azure/)
- [AVM Bicep Registry Index](https://azure.github.io/Azure-Verified-Modules/indexes/bicep/)

---

*Last Updated: February 22, 2026*
